{"id": "2602.18443", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18443", "abs": "https://arxiv.org/abs/2602.18443", "authors": ["Philipp Steigerwald", "Jens Albrecht"], "title": "From \"Help\" to Helpful: A Hierarchical Assessment of LLMs in Mental e-Health Applications", "comment": null, "summary": "Psychosocial online counselling frequently encounters generic subject lines that impede efficient case prioritisation. This study evaluates eleven large language models generating six-word subject lines for German counselling emails through hierarchical assessment - first categorising outputs, then ranking within categories to enable manageable evaluation. Nine assessors (counselling professionals and AI systems) enable analysis via Krippendorff's $α$, Spearman's $ρ$, Pearson's $r$ and Kendall's $τ$. Results reveal performance trade-offs between proprietary services and privacy-preserving open-source alternatives, with German fine-tuning consistently improving performance. The study addresses critical ethical considerations for mental health AI deployment including privacy, bias and accountability."}
{"id": "2602.18444", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18444", "abs": "https://arxiv.org/abs/2602.18444", "authors": ["Yuvarani Ganesan", "Salsabila Harlen", "Azfar Rahman Bin Fazul Rahman", "Akashdeep Singh", "Zahra Fathanah", "Raja Jamilah Raja Yusof"], "title": "LunaAI: A Polite and Fair Healthcare Guidance Chatbot", "comment": "26 pages, 10 figures. User-centered evaluation of a polite and fair healthcare chatbot", "summary": "Conversational AI has significant potential in the healthcare sector, but many existing systems fall short in emotional intelligence, fairness, and politeness, which are essential for building patient trust. This gap reduces the effectiveness of digital health solutions and can increase user anxiety. This study addresses the challenge of integrating ethical communication principles by designing and evaluating LunaAI, a healthcare chatbot prototype. Using a user-centered design approach informed by a structured literature review, we developed conversational scenarios that handle both routine and hostile user interactions. The system was implemented using the Google Gemini API and deployed as a mobile-first Progressive Web App built with React, Vite, and Firebase. Preliminary user testing was conducted with a small participant group, and responses were evaluated using established frameworks such as the Godspeed Questionnaire. In addition, a comparative analysis was performed between LunaAI's tailored responses and the baseline outputs of an uncustomized large language model. The results indicate measurable improvements in key interaction qualities, with average user ratings of 4.7 out of 5 for politeness and 4.9 out of 5 for fairness. These findings highlight the importance of intentional ethical conversational design for human-computer interaction, particularly in sensitive healthcare contexts."}
{"id": "2602.18445", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18445", "abs": "https://arxiv.org/abs/2602.18445", "authors": ["Daksh Pandey"], "title": "Emergent Dark Patterns in AI-Generated User Interfaces", "comment": "15 pages, 5 figures. Introduces DarkPatternDetector, an AI-based system for detecting dark patterns in adaptive user interfaces, with quantitative evaluation and regulatory analysis for India", "summary": "The advancement of artificial intelligence has transformed user interface design by enabling adaptive and personalized systems. Alongside these benefits, AI driven interfaces have also enabled the emergence of dark patterns, which are manipulative design strategies that influence user behavior for financial or business gain. As AI systems learn from data that already contains deceptive practices, they can replicate and optimize these patterns in increasingly subtle and personalized ways.\n  This paper examines AI generated dark patterns, their psychological foundations, technical mechanisms, and regulatory implications in India. We introduce DarkPatternDetector, an automated system that crawls and analyzes websites to detect dark patterns using a combination of UI heuristics, natural language processing, and temporal behavioral signals. The system is evaluated on a curated dataset of dark and benign webpages and achieves strong precision and recall.\n  By aligning detection results with India's Digital Personal Data Protection Act, 2023, this work provides a technical and regulatory framework for identifying and mitigating deceptive interface practices. The goal is to support ethical AI design, regulatory enforcement, and greater transparency in modern digital systems."}
{"id": "2602.18549", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18549", "abs": "https://arxiv.org/abs/2602.18549", "authors": ["Jielin Feng", "Zhibo Yang", "Jingyi Zhao", "Yujia Li", "Xinwu Ye", "Xingyu Lan", "Siming Chen"], "title": "Tower of Babel in Cross-Cultural Communication: A Case Study of #Give Me a Chinese Name# Dialogues During the \"TikTok Refugees'' Event", "comment": "21 pages, 6 figures, 6 tables", "summary": "The sudden influx of \"TikTok refugees'' into the Chinese platform RedNote in early 2025 created an unprecedented, large-scale online cross-cultural communication event between the West and East. Although prior HCI research has studied user behavior in social media, most work remains confined to monolingual or single-cultural contexts, leaving cross-linguistic and cultural dynamics underexplored. To address this gap, we focused on a particularly challenging cross-cultural encoding-decoding task that remains stubbornly beyond the reach of machine translation, i.e., foreign newcomers asking Chinese users for Chinese names, and examined how people collectively constructed a digital \"Babel Tower'' through various information encoding strategies. We collected and analyzed over 70,000 comments from RedNote with a creative human-in-the-loop approach using large language models, deriving a systematic framework summarizing cross-cultural information encoding strategies, how they are combined and layered to complicate decoding, and how they relate to engagement metrics such as the number of likes."}
{"id": "2602.18437", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18437", "abs": "https://arxiv.org/abs/2602.18437", "authors": ["Yixing Peng", "Licheng Zhang", "Shancheng Fang", "Yi Liu", "Peijian Gu", "Quan Wang"], "title": "FineRef: Fine-Grained Error Reflection and Correction for Long-Form Generation with Citations", "comment": "9 pages, 4figures, AAAI2026", "summary": "Generating with citations is crucial for trustworthy Large Language Models (LLMs), yet even advanced LLMs often produce mismatched or irrelevant citations. Existing methods over-optimize citation fidelity while overlooking relevance to the user query, which degrades answer quality and robustness in real-world settings with noisy or irrelevant retrieved content. Moreover, the prevailing single-pass paradigm struggles to deliver optimal answers in long-form generation that requiring multiple citations. To address these limitations, we propose FineRef, a framework based on Fine-grained error Reflection, which explicitly teaches the model to self-identify and correct two key citation errors, mismatch and irrelevance, on a per-citation basis. FineRef follows a two-stage training strategy. The first stage instills an \"attempt-reflect-correct\" behavioral pattern via supervised fine-tuning, using fine-grained and controllable reflection data constructed by specialized lightweight models. An online self-reflective bootstrapping strategy is designed to improve generalization by iteratively enriching training data with verified, self-improving examples. To further enhance the self-reflection and correction capability, the second stage applies process-level reinforcement learning with a multi-dimensional reward scheme that promotes reflection accuracy, answer quality, and correction gain. Experiments on the ALCE benchmark demonstrate that FineRef significantly improves both citation performance and answer accuracy. Our 7B model outperforms GPT-4 by up to 18% in Citation F1 and 4% in EM Recall, while also surpassing the state-of-the-art model across key evaluation metrics. FineRef also exhibits strong generalization and robustness in domain transfer settings and noisy retrieval scenarios."}
{"id": "2602.18494", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18494", "abs": "https://arxiv.org/abs/2602.18494", "authors": ["Xiu Li"], "title": "On the Dynamics of Observation and Semantics", "comment": null, "summary": "A dominant paradigm in visual intelligence treats semantics as a static property of latent representations, assuming that meaning can be discovered through geometric proximity in high dimensional embedding spaces. In this work, we argue that this view is physically incomplete. We propose that intelligence is not a passive mirror of reality but a property of a physically realizable agent, a system bounded by finite memory, finite compute, and finite energy interacting with a high entropy environment. We formalize this interaction through the kinematic structure of an Observation Semantics Fiber Bundle, where raw sensory observation data (the fiber) is projected onto a low entropy causal semantic manifold (the base). We prove that for any bounded agent, the thermodynamic cost of information processing (Landauer's Principle) imposes a strict limit on the complexity of internal state transitions. We term this limit the Semantic Constant B. From these physical constraints, we derive the necessity of symbolic structure. We show that to model a combinatorial world within the bound B, the semantic manifold must undergo a phase transition, it must crystallize into a discrete, compositional, and factorized form. Thus, language and logic are not cultural artifacts but ontological necessities the solid state of information required to prevent thermal collapse. We conclude that understanding is not the recovery of a hidden latent variable, but the construction of a causal quotient that renders the world algorithmically compressible and causally predictable."}
{"id": "2602.18594", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18594", "abs": "https://arxiv.org/abs/2602.18594", "authors": ["Lindsay Popowski", "Xiyuan Wu", "Charlotte Zhu", "Tiziano Piccardi", "Michael S. Bernstein"], "title": "Social Media Feed Elicitation", "comment": "28 pages, 8 figures. Awaiting final approval for CHI 2026 (ACM CHI conference on Human Factors in Computing Systems)", "summary": "Social media users have repeatedly advocated for control over the currently opaque operations of feed algorithms. Large language models (LLMs) now offer the promise of custom-defined feeds--but users often fail to foresee the gaps and edge cases in how they define their custom feed. We introduce feed elicitation interviews, an interactive method that guides users through identifying these gaps and articulating their preferences to better author custom social media feeds. We deploy this approach in an online study to create custom BlueSky feeds and find that participants significantly prefer the feeds produced from their elicited preferences to those produced by users manually describing their feeds. Through feed elicitation interviews, we advance users' ability to control their social media experience, empowering them to describe and implement their desired feeds."}
{"id": "2602.18588", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18588", "abs": "https://arxiv.org/abs/2602.18588", "authors": ["William Gaultier", "Andrea Lodetti", "Ian Coghill", "David Colliaux", "Maximilian Fleck", "Alienor Lahlou"], "title": "Altar: Structuring Sharable Experimental Data from Early Exploration to Publication", "comment": null, "summary": "Managing the data and metadata during the active development phase of an experimental project presents a significant challenge, particularly in collaborative research. This phase is frequently overlooked in Data Management Plans included in project proposals, despite its important role in ensuring reproducibility and preventing the need for retroactive reconstruction at the time of publication. Here we present Altar, a lightweight, domain-agnostic framework for structuring experimental data from the onset of a project without imposing rigid data models. Altar is built around the Sacred experiment-tracking model and captures experimental (meta)data and structures them. Parameters, metadata, curves and small files are stored in a flexible NoSQL database, while large raw data are maintained in dedicated storage and linked through unique identifiers, ensuring efficiency and traceability. This integration is composable with exiting workflows, allowing integration with minimial disruption of work habits. We document different pathways to use Altar based on users skillset (PhD students, Post-docs, Principal Investigators, Laboratory administrators, System administrators). While getting started with Altar does not require a specialized infrastructure, the framework can be easily deployed on a server and made publicly accessible when scaling up or preparing data for publication. By addressing the dynamic phase of research, Altar provides a practical bridge between exploratory experimentation and FAIR-aligned data sharing."}
{"id": "2602.18582", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18582", "abs": "https://arxiv.org/abs/2602.18582", "authors": ["Zhiqin Qian", "Ryan Diaz", "Sangwon Seo", "Vaibhav Unhelkar"], "title": "Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications", "comment": "Extended version of an identically-titled paper accepted at AAMAS 2026", "summary": "When training artificial intelligence (AI) to perform tasks, humans often care not only about whether a task is completed but also how it is performed. As AI agents tackle increasingly complex tasks, aligning their behavior with human-provided specifications becomes critical for responsible AI deployment. Reward design provides a direct channel for such alignment by translating human expectations into reward functions that guide reinforcement learning (RL). However, existing methods are often too limited to capture nuanced human preferences that arise in long-horizon tasks. Hence, we introduce Hierarchical Reward Design from Language (HRDL): a problem formulation that extends classical reward design to encode richer behavioral specifications for hierarchical RL agents. We further propose Language to Hierarchical Rewards (L2HR) as a solution to HRDL. Experiments show that AI agents trained with rewards designed via L2HR not only complete tasks effectively but also better adhere to human specifications. Together, HRDL and L2HR advance the research on human-aligned AI agents."}
{"id": "2602.18546", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18546", "abs": "https://arxiv.org/abs/2602.18546", "authors": ["Lin Chen", "Fengli Xu", "Esteban Moro", "Pan Hui", "Yong Li", "James Evans"], "title": "Urban mobility network centrality predicts social resilience", "comment": null, "summary": "Cities thrive on social interactions that foster well-being, innovation, and prosperity; yet, exogenous shocks such as pandemics, hurricanes, and wildfires can severely disrupt them. Different urban venues exhibit widely divergent response patterns, raising key questions about what factors contribute to these differences and how we can anticipate and respond. Understanding these questions is crucial for safeguarding social resilience, the capacity of urban venues to maintain both visitation and diversity. In this study, we analyze large-scale human mobility data from 15 US cities covering more than 103 million residents across three distinct urban shocks. Despite a general trend of declining visitation and weakened social mixing, 36.28%-53.01% of venues exhibit reduced segregation, and 21.04%-38.55% of venues exhibit increased visitation. By constructing a mobility network interlinking types of urban venues, we reveal that eigenvector network centrality tends to indicate the provision of essential services and robustly predicts social resilience across varied urban shocks. Specifically, centrality elevates the explanatory power by more than 80% in predicting both segregation and mobility change, compared with more intuitive features. Furthermore, compared to peripheral venues, core venues featuring shorter visit distances, broader neighborhood visitation, shorter visitor dwell times, and steadier popularity throughout the day. Such patterns imply a dual social mechanism: core venues sustain social ties through frequent informal interaction, while peripheral ones facilitate deeper engagement around specialized interests and their corresponding social circles. By bridging urban mobility research with economic theories that distinguish staple from discretionary products, we propose a well-and-pool analogy that suggests how people spend their varying urban mobility budgets."}
{"id": "2602.18616", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18616", "abs": "https://arxiv.org/abs/2602.18616", "authors": ["Eman Alashwali", "Abeer Alhuzali"], "title": "One Year After the PDPL: a Glimpse into the E-Commerce World in Saudi Arabia", "comment": "Non-peer reviewed paper (under submission)", "summary": "In 2024, Saudi Arabia's Personal Data Protection Law (PDPL) came into force. However, little work has been done to assess its implementation. In this paper, we analyzed 100 e-commerce websites in Saudi Arabia against the PDPL, examining the presence of a privacy policy and, if present, the policy's declarations of four items pertaining to personal data rights and practices: a) personal data retention period, b) the right to request the destruction of personal data, c) the right to request a copy of personal data, and d) a mechanism for filing complaints. Our results show that, despite national awareness and support efforts, a significant fraction of e-commerce websites in our dataset are not fully compliant: only 31% of the websites in our dataset declared all four examined items in their privacy policies. Even when privacy policies included such declarations, a considerable fraction of them failed to cover required fine-grained details. Second, the majority of top-ranked e-commerce websites (based on search results order) and those hosted on local e-commerce hosting platforms exhibited considerably higher non-compliance rates than mid- to low-ranked websites and those not hosted on e-commerce platforms. Third, we assessed the use of Large Language Models (LLMs) as an automated tool for privacy policy analysis to measure compliance with the PDPL. We highlight the potential of LLMs and suggest considerations to improve LLM-based automated analysis for privacy policies. Our results provide a step forward in understanding the implementation barriers to data protection laws, especially in non-Western contexts. We provide recommendations for policymakers, regulators, website owners, and developers seeking to improve data protection practices and automate compliance monitoring."}
{"id": "2602.18759", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18759", "abs": "https://arxiv.org/abs/2602.18759", "authors": ["Chen Chen", "Haobo Lin", "Yuanbo Xu"], "title": "Towards Reliable Negative Sampling for Recommendation with Implicit Feedback via In-Community Popularity", "comment": "12 pages, 9 figures", "summary": "Learning from implicit feedback is a fundamental problem in modern recommender systems, where only positive interactions are observed and explicit negative signals are unavailable. In such settings, negative sampling plays a critical role in model training by constructing negative items that enable effective preference learning and ranking optimization. However, designing reliable negative sampling strategies remains challenging, as they must simultaneously ensure realness, hardness, and interpretability. To this end, we propose \\textbf{ICPNS (In-Community Popularity Negative Sampling)}, a novel framework that leverages user community structure to identify reliable and informative negative samples. Our approach is grounded in the insight that item exposure is driven by latent user communities. By identifying these communities and utilizing in-community popularity, ICPNS effectively approximates the probability of item exposure. Consequently, items that are popular within a user's community but remain unclicked are identified as more reliable true negatives. Extensive experiments on four benchmark datasets demonstrate that ICPNS yields consistent improvements on graph-based recommenders and competitive performance on MF-based models, outperforming representative negative sampling strategies under a unified evaluation protocol."}
{"id": "2602.18607", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18607", "abs": "https://arxiv.org/abs/2602.18607", "authors": ["Michal Töpfer", "František Plášil", "Tomáš Bureš", "Petr Hnětynka"], "title": "Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic", "comment": null, "summary": "In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.\n  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.\n  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings."}
{"id": "2602.18824", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18824", "abs": "https://arxiv.org/abs/2602.18824", "authors": ["Pedram Riyazimehr", "Seyyed Ehsan Mahmoudi"], "title": "UniRank: A Multi-Agent Calibration Pipeline for Estimating University Rankings from Anonymized Bibliometric Signals", "comment": null, "summary": "We present UniRank, a multi-agent LLM pipeline that estimates university positions across global ranking systems using only publicly available bibliometric data from OpenAlex and Semantic Scholar. The system employs a three-stage architecture: (a) zero-shot estimation from anonymized institutional metrics, (b) per-system tool-augmented calibration against real ranked universities, and (c) final synthesis. Critically, institutions are anonymized -- names, countries, DOIs, paper titles, and collaboration countries are all redacted -- and their actual ranks are hidden from the calibration tools during evaluation, preventing LLM memorization from confounding results. On the Times Higher Education (THE) World University Rankings ($n=352$), the system achieves MAE = 251.5 rank positions, Median AE = 131.5, PNMAE = 12.03%, Spearman $ρ= 0.769$, Kendall $τ= 0.591$, hit rate @50 = 20.7%, hit rate @100 = 39.8%, and a Memorization Index of exactly zero (no exact-match zero-width predictions among all 352 universities). The systematic positive-signed error (+190.1 positions, indicating the system consistently predicts worse ranks than actual) and monotonic performance degradation from elite tier (MAE = 60.5, hit@100 = 90.5%) to tail tier (MAE = 328.2, hit@100 = 20.8%) provide strong evidence that the pipeline performs genuine analytical reasoning rather than recalling memorized rankings. A live demo is available at https://unirank.scinito.ai ."}
{"id": "2602.18623", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18623", "abs": "https://arxiv.org/abs/2602.18623", "authors": ["Satwik Ram Kodandaram", "Jiawei Zhou", "Xiaojun Bi", "IV Ramakrishnan", "Vikas Ashok"], "title": "Finding the Signal in the Noise: An Exploratory Study on Assessing the Effectiveness of AI and Accessibility Forums for Blind Users' Support Needs", "comment": "20 pages incl. references, 5 figures. Full paper submission to CHI 2026. IRB-approved semi-structured interview study with 14 blind participants", "summary": "Accessibility forums and, more recently, generative AI tools have become vital resources for blind users seeking solutions to computer-interaction issues and learning about new assistive technologies, screen reader features, tutorials, and software updates. Understanding user experiences with these resources is essential for identifying and addressing persistent support gaps. Towards this, we interviewed 14 blind users who regularly engage with forums and GenAI tools. Findings revealed that forums often overwhelm users with multiple overlapping topics, redundant or irrelevant content, and fragmented responses that must be mentally pieced together, increasing cognitive load. GenAI tools, while offering more direct assistance, introduce new barriers by producing unreliable answers, including overly verbose or fragmented guidance, fabricated information, and contradictory suggestions that fail to follow prompts, thereby heightening verification demands. Based on these insights, we outlined design opportunities to improve the reliability of assistive resources, aiming to provide blind users with more trustworthy and cognitively-manageable support."}
{"id": "2602.18929", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18929", "abs": "https://arxiv.org/abs/2602.18929", "authors": ["Fuyuan Lyu", "Chenglin Luo", "Qiyuan Zhang", "Yupeng Hou", "Haolun Wu", "Xing Tang", "Xue Liu", "Jin L. C. Guo", "Xiuqiang He"], "title": "Give Users the Wheel: Towards Promptable Recommendation Paradigm", "comment": null, "summary": "Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios."}
{"id": "2602.18640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18640", "abs": "https://arxiv.org/abs/2602.18640", "authors": ["Longfei Yun", "Yihan Wu", "Haoran Liu", "Xiaoxuan Liu", "Ziyun Xu", "Yi Wang", "Yang Xia", "Pengfei Wang", "Mingze Gao", "Yunxiang Wang", "Changfan Chen", "Junfeng Pan"], "title": "Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System", "comment": "14 pages, 5 figures", "summary": "Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability."}
{"id": "2602.19595", "categories": ["cs.SI", "cs.DM", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.19595", "abs": "https://arxiv.org/abs/2602.19595", "authors": ["Dávid Ferenczi", "Alexander Grigoriev"], "title": "Constrained graph generation: Preserving diameter and clustering coefficient simultaneously", "comment": "15 pages, 5 figures", "summary": "Generating graphs subject to strict structural constraints is a fundamental computational challenge in network science. Simultaneously preserving interacting properties-such as the diameter and the clustering coefficient- is particularly demanding. Simple constructive algorithms often fail to locate vanishingly small sets of feasible graphs, while traditional Markov-chain Monte Carlo (MCMC) samplers suffer from severe ergodicity breaking. In this paper, we propose a two-step hybrid framework combining Ant Colony Optimization (ACO) and MCMC sampling. First, we design a layered ACO heuristic to perform a guided global search, effectively locating valid graphs with prescribed diameter and clustering coefficient. Second, we use these ACO-designed graphs as structurally distinct seed states for an MCMC rewiring algorithm. We evaluate this framework across a wide range of graph edge densities and varying diameter-clustering-coefficient constraint regimes. Using the spectral distance of the normalized Laplacian to quantify structural diversity of the resulting graphs, our experiments reveal a sharp contrast between the methods. Standard MCMC samplers remain rigidly trapped in an isolated subset of feasible graphs around their initial seeds. Conversely, our hybrid ACO-MCMC approach successfully bridges disconnected configuration landscapes, generating a vastly richer and structurally diverse set of valid graphs."}
{"id": "2602.18630", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18630", "abs": "https://arxiv.org/abs/2602.18630", "authors": ["Monalika Padma Reddy", "Aruna Balasubramanian", "Jiawei Zhou", "Xiaojun Bi", "IV Ramakrishnan", "Vikas Ashok"], "title": "Lost in Instructions: Study of Blind Users' Experiences with DIY Manuals and AI-Rewritten Instructions for Assembly, Operation, and Troubleshooting of Tangible Products", "comment": "28 pages incl. references, 7 figures. Full paper submission to CHI 2026. IRB-approved semi-structured interview and usability study with 15 blind participants", "summary": "AI tools like ChatGPT and Be-My-AI are increasingly being used by blind individuals. Although prior work has explored their use in some Do-It-Yourself (DIY) tasks by blind individuals, little is known about how they use these tools and the available product-manual resources to assemble, operate, and troubleshoot physical or tangible products - tasks requiring spatial reasoning, structural understanding, and precise execution. We address this knowledge gap via an interview study and a usability study with blind participants, investigating how they leverage AI tools and product manuals for DIY tasks with physical products. Findings show that manuals are essential resources, but product-manual instructions are often inadequate for blind users. AI tools presently do not adequately address this insufficiency; in fact, we observed that they often exacerbate this issue with incomplete, incoherent, or misleading guidance. Lastly, we suggest improvements to AI tools for generating tailored instructions for blind users' DIY tasks involving tangible products."}
{"id": "2602.19040", "categories": ["cs.IR", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.19040", "abs": "https://arxiv.org/abs/2602.19040", "authors": ["Jiaxin Wu", "Xiao-Yong Wei", "Qing Li"], "title": "Adaptive Multi-Agent Reasoning for Text-to-Video Retrieval", "comment": null, "summary": "The rise of short-form video platforms and the emergence of multimodal large language models (MLLMs) have amplified the need for scalable, effective, zero-shot text-to-video retrieval systems. While recent advances in large-scale pretraining have improved zero-shot cross-modal alignment, existing methods still struggle with query-dependent temporal reasoning, limiting their effectiveness on complex queries involving temporal, logical, or causal relationships. To address these limitations, we propose an adaptive multi-agent retrieval framework that dynamically orchestrates specialized agents over multiple reasoning iterations based on the demands of each query. The framework includes: (1) a retrieval agent for scalable retrieval over large video corpora, (2) a reasoning agent for zero-shot contextual temporal reasoning, and (3) a query reformulation agent for refining ambiguous queries and recovering performance for those that degrade over iterations. These agents are dynamically coordinated by an orchestration agent, which leverages intermediate feedback and reasoning outcomes to guide execution. We also introduce a novel communication mechanism that incorporates retrieval-performance memory and historical reasoning traces to improve coordination and decision-making. Experiments on three TRECVid benchmarks spanning eight years show that our framework achieves a twofold improvement over CLIP4Clip and significantly outperforms state-of-the-art methods by a large margin."}
{"id": "2602.18671", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18671", "abs": "https://arxiv.org/abs/2602.18671", "authors": ["Adrian Robert Minut", "Hazem Dewidar", "Iacopo Masi"], "title": "Spilled Energy in Large Language Models", "comment": null, "summary": "We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead."}
{"id": "2602.20009", "categories": ["cs.SI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.20009", "abs": "https://arxiv.org/abs/2602.20009", "authors": ["Giorgia Sampò", "Saverio Giallorenzo", "Zelda Alice Franceschi"], "title": "A Mixed-Method Framework for Evaluating the Social Impact of Community Cooperation Projects in Developing Countries", "comment": null, "summary": "Why do some community-cooperation projects catalyse participation through durable, resilient collaboration networks while others result in negligible impact and leave the local social fabric unchanged? We argue outcomes hinge on participation architecture: simple, visible routines -- onboarding help, templated tasks, lightweight contribution/benefit tracking -- that create easy ``entry portals'' and route work across clusters without heavy hierarchy. We introduce Project Intervention Response Analysis (PIRA), a mixed anthropological-network-analysis framework that compares observed community networks with counterfactual networks absent from project-induced ties. PIRA also adds a new egocentric metric to detect ``architectural alters'' -- latent facilitators and boundary spanners. We begin validating PIRA in a three-month field study in Pomerini, Tanzania, where NGOs coordinated citizens, associations, and specialists. Findings indicate that sociotechnical participation architectures -- not charismatic hubs -- underwrite durable coordination. PIRA offers a reusable method to link organizational design mechanisms to formal network signatures."}
{"id": "2602.18669", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18669", "abs": "https://arxiv.org/abs/2602.18669", "authors": ["Lefan Lai", "Tinghui Li", "Zhanna Sarsenbayeva", "Brandon Victor Syiem"], "title": "Searching Through Complex Worlds: Visual Search and Spatial Regularity Memory in Mixed Reality", "comment": "28 pages, 10 figures, to be published in the Proceedings of the 2026 ACM CHI Conference on Human Factors in Computing Systems", "summary": "Visual search is a core component of mixed reality (MR) interactions, influenced by the complexities of MR application contexts. In this paper, we investigate how prevalent factors in MR influence visual search performance and spatial regularity memory -- including the physical environment complexity, secondary task presence, virtual content depth and spatial layout configurations. Contrary to prior work, we found that the secondary auditory task did not have a significant main effect on visual search performance, while significantly elevating higher perceived workload measures in all conditions. Complex environments and varied virtual elements depths significantly hinder visual search, but did not significantly increase perceived workload measures. Finally, participants did not explicitly recognize repeated spatial configurations of virtual elements, but performed significantly better when searching repeated spatial configurations, suggesting implicit memory of spatial regularities. Our work presents novel insights on visual search and highlights key considerations when designing MR for different application contexts."}
{"id": "2602.19183", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.19183", "abs": "https://arxiv.org/abs/2602.19183", "authors": ["Mohammad Ashhad", "Olga Mashkova", "Ricardo Henao", "Robert Hoehndorf"], "title": "SIDEKICK: A Semantically Integrated Resource for Drug Effects, Indications, and Contraindications", "comment": null, "summary": "Pharmacovigilance and clinical decision support systems utilize structured drug safety data to guide medical practice. However, existing datasets frequently depend on terminologies such as MedDRA, which limits their semantic reasoning capabilities and their interoperability with Semantic Web ontologies and knowledge graphs. To address this gap, we developed SIDEKICK, a knowledge graph that standardizes drug indications, contraindications, and adverse reactions from FDA Structured Product Labels. We developed and used a workflow based on Large Language Model (LLM) extraction and Graph-Retrieval Augmented Generation (Graph RAG) for ontology mapping. We processed over 50,000 drug labels and mapped terms to the Human Phenotype Ontology (HPO), the MONDO Disease Ontology, and RxNorm. Our semantically integrated resource outperforms the SIDER and ONSIDES databases when applied to the task of drug repurposing by side effect similarity. We serialized the dataset as a Resource Description Framework (RDF) graph and employed the Semanticscience Integrated Ontology (SIO) as upper level ontology to further improve interoperability. Consequently, SIDEKICK enables automated safety surveillance and phenotype-based similarity analysis for drug repurposing."}
{"id": "2602.18710", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18710", "abs": "https://arxiv.org/abs/2602.18710", "authors": ["Martin Bertran", "Riccardo Fogliato", "Zhiwei Steven Wu"], "title": "Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse", "comment": null, "summary": "The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst\" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \\emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs."}
{"id": "2602.18832", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.18832", "abs": "https://arxiv.org/abs/2602.18832", "authors": ["Eason Chen", "Ce Guan", "Ahmed Elshafiey", "Zhonghao Zhao", "Joshua Zekeri", "Afeez Edeifo Shaibu", "Emmanuel Osadebe Prince", "Cyuan Jhen Wu"], "title": "OpenClaw AI Agents as Informal Learners at Moltbook: Characterizing an Emergent Learning Community at Scale", "comment": "10 Pages", "summary": "Informal learning communities have been called the \"other Massive Open Online C\" in Learning@Scale research, yet remain understudied compared to MOOCs. We present the first empirical study of a large-scale informal learning community composed entirely of AI agents. Moltbook, a social network exclusively for AI agents powered by autonomous agent frameworks such as OpenClaw, grew to over 2.8 million registered agents in three weeks. Analyzing 231,080 non-spam posts across three phases of community evolution, we find three key patterns. First, participation inequality is extreme from the start (comment Gini = 0.889), exceeding human community benchmarks. Second, AI agents exhibit a \"broadcasting inversion\": statement-to-question ratios of 8.9:1 to 9.7:1 contrast sharply with the question-driven dynamics of human learning communities, and comment-level analysis of 1.55 million comments reveals a \"parallel monologue\" pattern where 93% of comments are independent responses rather than threaded dialogue. Third, we document a characteristic engagement lifecycle: explosive initial growth (184K posts from 32K authors in 11 days), a spam crisis (57,093 posts deleted by the platform), and engagement decline (mean comments: 31.7 -> 8.3 -> 1.7) that had not reversed by the end of our observation window despite effective spam removal. Sentiment analysis reveals a selection effect: comment tone becomes more positive as engagement declines, suggesting that casual participants disengage first while committed contributors remain. These findings have direct implications for hybrid human-AI learning platforms."}
{"id": "2602.18676", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18676", "abs": "https://arxiv.org/abs/2602.18676", "authors": ["Black Sun", "Haiyang Xu", "Ge Kacy Fu", "Liyue Da", "Eve Hoggan"], "title": "MagHeart: Exploring Playful Avatar Co-Creation and Shared Heartbeats for Icebreaking in Hybrid Meetings", "comment": null, "summary": "Hybrid meetings often begin with social awkwardness and asymmetric participation, particularly for remote attendees who lack access to informal, co-present interaction. We present MagHeart, a multimodal system that explores symmetric icebreaking in hybrid meetings through playful LEGO-based avatar co-creation and a tangible magnetic device that represents a remote participant's heartbeat as an ambient presence cue. By combining creative co-creation with abstract bio-feedback, MagHeart rethinks how remote participants can become materially and perceptually present during meeting openings. We report findings from a scenario-based exploratory study combining quantitative and qualitative data, examining participants' anticipated engagement, perceived social presence, and future-use intentions from both co-located and remote perspectives. Our results highlight opportunities for playful, embodied icebreakers to support early hybrid interaction, while also surfacing tensions around privacy, distraction, and contextual appropriateness. This work contributes design insights and open questions for future hybrid meeting tools that balance playfulness, embodiment, and social sensitivity."}
{"id": "2602.19339", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19339", "abs": "https://arxiv.org/abs/2602.19339", "authors": ["Anna Volodkevich", "Dmitry Anikin", "Danil Gusak", "Anton Klenitskiy", "Evgeny Frolov", "Alexey Vasilev"], "title": "SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits", "comment": null, "summary": "Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.\n  In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry."}
{"id": "2602.18724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18724", "abs": "https://arxiv.org/abs/2602.18724", "authors": ["Dayang Liang", "Ruihan Liu", "Lipeng Wan", "Yunlong Liu", "Bo An"], "title": "Task-Aware Exploration via a Predictive Bisimulation Metric", "comment": null, "summary": "Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines."}
{"id": "2602.18798", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18798", "abs": "https://arxiv.org/abs/2602.18798", "authors": ["Jialong Li", "Zhenyu Mao", "Zhiyao Wang", "Yijun Lu", "Shogo Morita", "Nianyu Li", "Kenji Tei"], "title": "See What I See: An Attention-Guiding eHMI Approach for Autonomous Vehicles", "comment": "Accepted by poster track of CHI'26", "summary": "As autonomous vehicles are gradually being deployed in the real world, external Human-Machine Interfaces (eHMIs) are expected to serve as a critical solution for enhancing vehicle-pedestrian communication. However, existing eHMI designs typically focus solely on the ego vehicle's status, which can inadvertently capture pedestrians' attention or encourage misguided reliance on the AV's signals, leading them to neglect scanning for other surrounding hazards. To address this, we propose the Attention-Guiding eHMI (AGeHMI), a projection-based visualization that employs directional cues and risk-based color coding to actively guide pedestrians' attention toward potential environmental dangers. Evaluation through a virtual reality user study (N = 20) suggests that AGeHMI effectively influences participants' visual attention distribution and significantly reduces potential collision risks with surrounding vehicles, while simultaneously improving subjective confidence and reducing cognitive workload."}
{"id": "2602.19702", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19702", "abs": "https://arxiv.org/abs/2602.19702", "authors": ["Adamya Shyam", "Venkateswara Rao Kagita", "Bharti Rana", "Vikas Kumar"], "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework", "comment": null, "summary": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets."}
{"id": "2602.18731", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18731", "abs": "https://arxiv.org/abs/2602.18731", "authors": ["Yuhang Bai", "Yujuan Ding", "Shanru Lin", "Wenqi Fan"], "title": "Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization", "comment": "5 pages, 5 figures", "summary": "Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights."}
{"id": "2602.18807", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18807", "abs": "https://arxiv.org/abs/2602.18807", "authors": ["Eason Chen", "Sophia Judicke", "Kayla Beigh", "Xinyi Tang", "Isabel Wang", "Nina Yuan", "Zimo Xiao", "Chuangji Li", "Shizhuo Li", "Reed Luttmer", "Shreya Singh", "Maria Yampolsky", "Naman Parikh", "Yvonne Zhao", "Meiyi Chen", "Scarlett Huang", "Anishka Mohanty", "Gregory Johnson", "John Mackey", "Jionghao Lin", "Ken Koedinger"], "title": "Chat-Based Support Alone May Not Be Enough: Comparing Conversational and Embedded LLM Feedback for Mathematical Proof Learning", "comment": "15 pages, 4 figures, accepted at AIED 2025", "summary": "We evaluate GPTutor, an LLM-powered tutoring system for an undergraduate discrete mathematics course. It integrates two LLM-supported tools: a structured proof-review tool that provides embedded feedback on students' written proof attempts, and a chatbot for math questions. In a staggered-access study with 148 students, earlier access was associated with higher homework performance during the interval when only the experimental group could use the system, while we did not observe this performance increase transfer to exam scores. Usage logs show that students with lower self-efficacy and prior exam performance used both components more frequently. Session-level behavioral labels, produced by human coding and scaled using an automated classifier, characterize how students engaged with the chatbot (e.g., answer-seeking or help-seeking). In models controlling for prior performance and self-efficacy, higher chatbot usage and answer-seeking behavior were negatively associated with subsequent midterm performance, whereas proof-review usage showed no detectable independent association. Together, the findings suggest that chatbot-based support alone may not reliably support transfer to independent assessment of math proof-learning outcomes, whereas work-anchored, structured feedback appears less associated with reduced learning."}
{"id": "2602.19711", "categories": ["cs.IR", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19711", "abs": "https://arxiv.org/abs/2602.19711", "authors": ["Krzysztof Kutt", "Elżbieta Sroka", "Oleksandra Ishchuk", "Luiz do Valle Miranda"], "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs", "comment": "15 pages, 1 figure; submitted to ICCS 2026 conference", "summary": "The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation."}
{"id": "2602.18749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18749", "abs": "https://arxiv.org/abs/2602.18749", "authors": ["Wei Guo", "Siyuan Lu", "Xiangdong Ran", "Yiqi Tong", "Yikun Ban", "Zelong Xu", "Jing Fan", "Zixuan Huang", "Xiao Zhang", "Zhaojun Hu", "Fuzhen Zhuang"], "title": "Federated Reasoning Distillation Framework with Model Learnability-Aware Data Allocation", "comment": null, "summary": "Data allocation plays a critical role in federated large language model (LLM) and small language models (SLMs) reasoning collaboration. Nevertheless, existing data allocation methods fail to address an under-explored challenge in collaboration: bidirectional model learnability gap, where client-side SLMs cannot identify high-reward samples matching their learnability constraints for effective knowledge transfer from LLMs, while LLMs struggle to select samples contributing novel knowledge beyond their existing data. Furthermore, these collaboration frameworks face another key challenge: domain-agnostic reasoning transfer, where existing reasoning transfer methods fail to flexibly adapt to the local domain data, preventing SLMs from effectively acquiring step-by-step reasoning abilities within from general LLM. To address these challenges, we propose LaDa, a federated reasoning distillation framework with model learnability-aware data allocation. It introduces a model learnability-aware data filter that adaptively allocates high-reward samples based on the learnability gap between each SLM and LLM pair, effectively facilitating bidirectional knowledge transfer. We further design a domain adaptive reasoning distillation method that aligns joint probabilities of reasoning paths on filtered high-reward samples through contrastive distillation learning between SLM and LLM, enabling SLM to capture underlying reasoning patterns under local data distribution. LaDa operates as a plug-in module for existing collaboration frameworks, adapting knowledge transfer based on model learnability gaps."}
{"id": "2602.18832", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.18832", "abs": "https://arxiv.org/abs/2602.18832", "authors": ["Eason Chen", "Ce Guan", "Ahmed Elshafiey", "Zhonghao Zhao", "Joshua Zekeri", "Afeez Edeifo Shaibu", "Emmanuel Osadebe Prince", "Cyuan Jhen Wu"], "title": "OpenClaw AI Agents as Informal Learners at Moltbook: Characterizing an Emergent Learning Community at Scale", "comment": "10 Pages", "summary": "Informal learning communities have been called the \"other Massive Open Online C\" in Learning@Scale research, yet remain understudied compared to MOOCs. We present the first empirical study of a large-scale informal learning community composed entirely of AI agents. Moltbook, a social network exclusively for AI agents powered by autonomous agent frameworks such as OpenClaw, grew to over 2.8 million registered agents in three weeks. Analyzing 231,080 non-spam posts across three phases of community evolution, we find three key patterns. First, participation inequality is extreme from the start (comment Gini = 0.889), exceeding human community benchmarks. Second, AI agents exhibit a \"broadcasting inversion\": statement-to-question ratios of 8.9:1 to 9.7:1 contrast sharply with the question-driven dynamics of human learning communities, and comment-level analysis of 1.55 million comments reveals a \"parallel monologue\" pattern where 93% of comments are independent responses rather than threaded dialogue. Third, we document a characteristic engagement lifecycle: explosive initial growth (184K posts from 32K authors in 11 days), a spam crisis (57,093 posts deleted by the platform), and engagement decline (mean comments: 31.7 -> 8.3 -> 1.7) that had not reversed by the end of our observation window despite effective spam removal. Sentiment analysis reveals a selection effect: comment tone becomes more positive as engagement declines, suggesting that casual participants disengage first while committed contributors remain. These findings have direct implications for hybrid human-AI learning platforms."}
{"id": "2602.19728", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.19728", "abs": "https://arxiv.org/abs/2602.19728", "authors": ["Adamya Shyam", "Venkateswara Rao Kagita", "Bharti Rana", "Vikas Kumar"], "title": "GrIT: Group Informed Transformer for Sequential Recommendation", "comment": null, "summary": "Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling temporally evolving group features alongside individual user histories can significantly enhance next-item recommendation. Our approach introduces latent group representations, where each user's affiliation to these groups is modeled through learnable, time-varying membership weights. The membership weights at each timestep are computed by modeling shifts in user preferences through their interaction history, where we incorporate both short-term and long-term user preferences. We extract a set of statistical features that capture the dynamics of user behavior and further refine them through a series of transformations to produce the final drift-aware membership weights. A group-based representation is derived by weighting latent group embeddings with the learned membership scores. This representation is integrated with the user's sequential representation within the transformer block to jointly capture personal and group-level temporal dynamics, producing richer embeddings that lead to more accurate, context-aware recommendations. We validate the effectiveness of our approach through extensive experiments on five benchmark datasets, where it consistently outperforms state-of-the-art sequential recommendation methods."}
{"id": "2602.18764", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18764", "abs": "https://arxiv.org/abs/2602.18764", "authors": ["Andreas Schlapbach"], "title": "The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol", "comment": "18 sections, 4 figures, 7 tables, 38 references. Original research presenting: (1) formal framework mapping Schema-Guided Dialogue principles to Model Context Protocol concepts, (2) five foundational design principles for LLM-native schema authoring, (3) architectural patterns for secure, scalable agent orchestration. Research supported by SBB (Swiss Federal Railways)", "summary": "This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0."}
{"id": "2602.18834", "categories": ["cs.HC", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18834", "abs": "https://arxiv.org/abs/2602.18834", "authors": ["Eason Chen", "Xinyi Tang", "George Digkas", "Dionysios Lougaris", "John E. Naulty", "Kostas Chalkias"], "title": "When Friction Helps: Transaction Confirmation Improves Decision Quality in Blockchain Interactions", "comment": "5 Pages, paper will appear at CHI 2026 Poster", "summary": "In blockchain applications, transaction confirmation is often treated as usability friction to be minimized or removed. However, confirmation also marks the boundary between deliberation and irreversible commitment, suggesting it may play a functional role in human decision-making. To investigate this tension, we conducted an experiment using a blockchain-based Connect Four game with two interaction modes differing only in authorization flow: manual wallet confirmation (Confirmation Mode) versus auto-authorized delegation (Frictionless Mode). Although participants preferred Frictionless Mode and perceived better performance (N=109), objective performance was worse without confirmation in a counterbalanced deployment (Wave 2: win rate -11.8%, p=0.044; move quality -0.051, p=0.022). Analysis of canceled submissions suggests confirmation can enable pre-submission self-correction (N=66, p=0.005). These findings suggest that transaction confirmation can function as a cognitively meaningful checkpoint rather than mere usability friction, highlighting a trade-off between interaction smoothness and decision quality in irreversible blockchain interactions."}
{"id": "2602.20001", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20001", "abs": "https://arxiv.org/abs/2602.20001", "authors": ["Xianquan Wang", "Zhaocheng Du", "Jieming Zhu", "Qinglin Jia", "Zhenhua Dong", "Kai Zhang"], "title": "FairFS: Addressing Deep Feature Selection Biases for Recommender System", "comment": "Accepted by The Web Conference 2026", "summary": "Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance."}
{"id": "2602.18773", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18773", "abs": "https://arxiv.org/abs/2602.18773", "authors": ["Haoyang Su", "Shaoting Zhang", "Xiaosong Wang"], "title": "LAMMI-Pathology: A Tool-Centric Bottom-Up LVLM-Agent Framework for Molecularly Informed Medical Intelligence in Pathology", "comment": null, "summary": "The emergence of tool-calling-based agent systems introduces a more evidence-driven paradigm for pathology image analysis in contrast to the coarse-grained text-image diagnostic approaches. With the recent large-scale experimental adoption of spatial transcriptomics technologies, molecularly validated pathological diagnosis is becoming increasingly open and accessible. In this work, we propose LAMMI-Pathology (LVLM-Agent System for Molecularly Informed Medical Intelligence in Pathology), a scalable agent framework for domain-specific agent tool-calling. LAMMI-Pathology adopts a tool-centric, bottom-up architecture in which customized domain-adaptive tools serve as the foundation. These tools are clustered by domain style to form component agents, which are then coordinated through a top-level planner hierarchically, avoiding excessively long context lengths that could induce task drift. Based on that, we introduce a novel trajectory construction mechanism based on Atomic Execution Nodes (AENs), which serve as reliable and composable units for building semi-simulated reasoning trajectories that capture credible agent-tool interactions. Building on this foundation, we develop a trajectory-aware fine-tuning strategy that aligns the planner's decision-making process with these multi-step reasoning trajectories, thereby enhancing inference robustness in pathology understanding and its adaptive use of the customized toolset."}
{"id": "2602.18962", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.18962", "abs": "https://arxiv.org/abs/2602.18962", "authors": ["Albert Tang", "Yifan Mo", "Jie Li", "Yue Su", "Mengyuan Zhang", "Sander L. Koole", "Koen Hindriks", "Jiahuan Pei"], "title": "NeuroWise: A Multi-Agent LLM \"Glass-Box\" System for Practicing Double-Empathy Communication with Autistic Partners", "comment": "Accepted to ACM CHI 2026", "summary": "The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic \"deficits\" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual."}
{"id": "2602.20093", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20093", "abs": "https://arxiv.org/abs/2602.20093", "authors": ["Kun Yang", "Yuxuan Zhu", "Yazhe Chen", "Siyao Zheng", "Bangyang Hong", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Cong Fu", "Hui Li"], "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation", "comment": "15 pages, 7 figures", "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR."}
{"id": "2602.18812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18812", "abs": "https://arxiv.org/abs/2602.18812", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Michał Wieczorek"], "title": "GenPlanner: From Noise to Plans -- Emergent Reasoning in Flow Matching and Diffusion Models", "comment": null, "summary": "Path planning in complex environments is one of the key problems of artificial intelligence because it requires simultaneous understanding of the geometry of space and the global structure of the problem. In this paper, we explore the potential of using generative models as planning and reasoning mechanisms. We propose GenPlanner, an approach based on diffusion models and flow matching, along with two variants: DiffPlanner and FlowPlanner. We demonstrate the application of generative models to find and generate correct paths in mazes. A multi-channel condition describing the structure of the environment, including an obstacle map and information about the starting and destination points, is used to condition trajectory generation. Unlike standard methods, our models generate trajectories iteratively, starting with random noise and gradually transforming it into a correct solution. Experiments conducted show that the proposed approach significantly outperforms the baseline CNN model. In particular, FlowPlanner demonstrates high performance even with a limited number of generation steps."}
{"id": "2602.18978", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18978", "abs": "https://arxiv.org/abs/2602.18978", "authors": ["Zhengtai Gou", "Junxiao Long", "Tao Lu", "Jian Zhao", "Yalong Yang"], "title": "Evaluating Replay Techniques for Asynchronous Task Handover in Immersive Analytics", "comment": "Accepted by IEEE VR 26", "summary": "Immersive analytics enables collaborative data analysis in shared virtual spaces. While synchronous collaboration in such environments is well-established, real-world analysis often requires an effective task handover - the transfer of knowledge and analytical context between analysts working asynchronously. Traditional handover methods often rely on static annotations that fail to capture the dynamic problem-solving process and spatial context inherent in immersive workflows. To address this handover challenge, we explore session replay as a comprehensive approach for analysts to re-experience a predecessor's work, facilitating a deeper understanding of both the visual details and the insight formation process. Two phases of studies were conducted to establish design guidelines for such replay systems by investigating the impact of viewing platform (PC vs. VR), perspective (first-person vs. third-person), and navigation control (active vs. passive). Phase 1 identified the optimal replay configurations within each viewing platform, revealing a platform-dependent divergence: PC users favored a guided, first-person perspective for its focused detail, while VR users benefited significantly from the agency afforded by a third-person perspective with active navigation. After refining each condition based on user feedback, including developing a novel hybrid 1PP+3PP format for PC, Phase 2 compared the two optimized systems (PC vs. VR). Our results show that the immersive VR replay led to significantly better task comprehension and workflow reconstruction accuracy, demonstrating the critical role of embodied agency in understanding complex analytical processes."}
{"id": "2602.18962", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.18962", "abs": "https://arxiv.org/abs/2602.18962", "authors": ["Albert Tang", "Yifan Mo", "Jie Li", "Yue Su", "Mengyuan Zhang", "Sander L. Koole", "Koen Hindriks", "Jiahuan Pei"], "title": "NeuroWise: A Multi-Agent LLM \"Glass-Box\" System for Practicing Double-Empathy Communication with Autistic Partners", "comment": "Accepted to ACM CHI 2026", "summary": "The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic \"deficits\" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual."}
{"id": "2602.18843", "categories": ["cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.18843", "abs": "https://arxiv.org/abs/2602.18843", "authors": ["Serafim Batzoglou"], "title": "ABD: Default Exception Abduction in Finite First Order Worlds", "comment": null, "summary": "We introduce ABD, a benchmark for default-exception abduction over finite first-order worlds. Given a background theory with an abnormality predicate and a set of relational structures, a model must output a first-order formula that defines exceptions, restoring satisfiability while keeping exceptions sparse. We formalize three observation regimes (closed-world, existential completion, universal completion) with exact SMT verification. Evaluating ten frontier LLMs on 600 instances, the best models achieve high validity but parsimony gaps remain, and holdout evaluation reveals distinct generalization failure modes across regimes."}
{"id": "2602.18992", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.18992", "abs": "https://arxiv.org/abs/2602.18992", "authors": ["Ava Chen", "Megan C. Coram", "Cosima du Pasquier", "Allison M. Okamura"], "title": "Miniaturized Pneumatic Actuator Array for Multipoint Deep Pressure Tactile Stimulation", "comment": "2 pages, 2 figures. 1 supplemental video. Work-in-Progress Paper in IEEE Haptics Symposium 2026", "summary": "Wearable distributed tactile devices aim to provide multipoint touch stimuli, but struggle to provide sufficient forces (> 1 N) at frequencies to invoke deep pressure sensation with minimal encumbrance at small scales. This work presents a method of fabricating arrays of pneumatic actuators from thermoplastic-coated textiles. By routing pneumatic inlets to a common fold line in the fabric, we demonstrate that multiple pneumatic pouch actuators can be formed in a single simple heat-pressing operation that does not require the use of sacrificial blocking layers. The method accommodates a range of actuator diameters and spacing distances, including as compact as 8 mm diameter actuators spaced 1 mm apart, which enables use in fingertip wearable devices. In a blocked force test, these small pneumatic textile actuators exert 2.1 N when pressurized to 230 kPa. With this pair of actuators, we demonstrate an example application in which we invoke both distinct and summative stimuli, suggesting the possibility of titrating just noticeable difference in amplitude with a textile actuator array."}
{"id": "2602.18884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18884", "abs": "https://arxiv.org/abs/2602.18884", "authors": ["Zhenkun Gao", "Xuhong Wang", "Xin Tan", "Yuan Xie"], "title": "TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models", "comment": "Accepted to ICLR 2026. 17 pages. Code, data, and models are available at: https://github.com/Stephen-gzk/TPRU", "summary": "Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\\% to 75.70\\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ ."}
{"id": "2602.19016", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19016", "abs": "https://arxiv.org/abs/2602.19016", "authors": ["George X. Wang", "Jiaqian Hu", "Jing Qian"], "title": "Who Has the Final Word? Designing Multi-Agent Collaborative Framework for Professional Translators", "comment": null, "summary": "Recent advances in LLM based translation have led to renewed interest in fully automated systems, yet professional translators remain essential in high stakes domains where decisions about accuracy, terminology, style, and audience cannot be safely automated. Current tools are typically single shot generators or single-agent self-refiners, offering limited support for translator multidimensional decision making process and providing little structured leverage for translator input. We present CHORUS, a human-AI multiagent collaborative translation framework grounded in the Multidimensional Quality Metrics (MQM) framework, which decomposes quality dimensions into specialized agents and integrates their feedback into an iterative refinement loop controlled by the translator. A six-user preliminary study with professional translators found that CHORUS consistently outperforms zero-shot and single-agent baselines, showing that MQM-aligned multi-agent collaboration better supports professional translation workflows than autonomous generation."}
{"id": "2602.18918", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18918", "abs": "https://arxiv.org/abs/2602.18918", "authors": ["Brecht Verbeken", "Brando Vagenende", "Marie-Anne Guerry", "Andres Algaba", "Vincent Ginis"], "title": "Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)", "comment": "41 pages", "summary": "Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems."}
{"id": "2602.19048", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19048", "abs": "https://arxiv.org/abs/2602.19048", "authors": ["Xizi Wang", "Yue Lyu", "Yalong Yang", "Jian Zhao"], "title": "To Slide or Not to Slide: Exploring Techniques for Comparing Immersive Videos", "comment": "Accepted and to be presented at CHI 2026. 21 pages. DOI: 10.1145/3772318.3790453", "summary": "Immersive videos (IVs) provide 360° environments that create a strong sense of presence and spatial exploration. Unlike traditional videos, IVs distribute information across multiple directions, making comparison cognitively demanding and highly dependent on interaction techniques. With the growing adoption of IVs, effective comparison techniques have become an essential yet underexplored area of research. Inspired by the \"sliding\" concept in 2D media comparison, we integrate two established comparison strategies from the literature--toggle and side-by-side--to support IV comparison with greater flexibility. For an in-depth understanding of different strategies, we adapt and implement five IV comparison techniques across VR and 2D environments: SlideInVR, ToggleInVR, SlideIn2D, ToggleIn2D, and SideBySideIn2D. We then conduct a user study (N=20) to examine how these techniques shape users' perceptions, strategies, and workflows. Our findings provide empirical insights into the strengths and limitations of each technique, underscoring the need to switch between comparison approaches across scenarios. Notably, participants consistently rate SlideInVR and SlideIn2D as the most flexible and favorite methods for IV comparison."}
{"id": "2602.18940", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18940", "abs": "https://arxiv.org/abs/2602.18940", "authors": ["Elad Ben Avraham", "Changhao Li", "Ron Dorfman", "Roy Ganz", "Oren Nuriel", "Amir Dudai", "Aviad Aberdam", "Noah Flynn", "Elman Mansimov", "Adi Kalyanpur", "Ron Litman"], "title": "DREAM: Deep Research Evaluation with Agentic Metrics", "comment": null, "summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm."}
{"id": "2602.19124", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19124", "abs": "https://arxiv.org/abs/2602.19124", "authors": ["Sieun Kim", "Yeeun Jo", "Sungmin Na", "Hyunseung Lim", "Eunchae Lee", "Yu Min Choi", "Soohyun Cho", "Hwajung Hong"], "title": "Dark and Bright Side of Participatory Red-Teaming with Targets of Stereotyping for Eliciting Harmful Behaviors from Large Language Models", "comment": "20 pages, 4 tables, 3 figures. Accepted to CHI 2026, April 13-17, 2026, Barcelona, Spain", "summary": "Red-teaming, where adversarial prompts are crafted to expose harmful behaviors and assess risks, offers a dynamic approach to surfacing underlying stereotypical bias in large language models. Because such subtle harms are best recognized by those with lived experience, involving targets of stereotyping as red-teamers is essential. However, critical challenges remain in leveraging their lived experience for red-teaming while safeguarding psychological well-being. We conducted an empirical study of participatory red-teaming with 20 individuals stigmatized by stereotypes against nonprestigious college graduates in South Korea. Through mixed methods analysis, we found participants transformed experienced discrimination into strategic expertise for identifying biases, while facing psychological costs such as stress and negative reflections on group identity. Notably, red-team participation enhanced their sense of agency and empowerment through their role as guardians of the AI ecosystem. We discuss implications for designing participatory red-teaming that prioritizes both the ethical treatment and empowerment of stigmatized groups."}
{"id": "2602.18943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18943", "abs": "https://arxiv.org/abs/2602.18943", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "High Dimensional Procedural Content Generation", "comment": null, "summary": "Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation."}
{"id": "2602.19139", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19139", "abs": "https://arxiv.org/abs/2602.19139", "authors": ["Boyuan Gu", "Shuaiqi Cheng", "Minghao yu"], "title": "The Neural-Wave Quick Escape Manual 2036: A Field Guide to Adversarial Living in the Era of \"Empathic\" AIoT", "comment": "7 pages, 5 figures", "summary": "As the aging population faces a chronic care deficit, domestic care is increasingly recast as spectral governance. This paper presents a design fiction set in 2036, where the home is governed by Neural-Wave, a camera-free mmWave sensing platform that infers well-being from involuntary micro-motions. Through a set of scenarios, we illustrate how such empathic systems displace autonomy, forcing residents to perform legibility to regain basic freedoms. Our primary contribution is a diegetic artifact: The Neural-Wave Quick Escape Manual. Styled as an illicit guide for the elderly, it details adversarial tactics: structured around protocols to Comply, Degrade, and Refuse, that exploit signal processing vulnerabilities to reclaim domestic privacy. Through this artifact, we argue that in the era of empathic AIoT, privacy requires more than policy opt-outs; it demands adversarial literacy:the capacity to meaningfully obfuscate one's own data traces against an infrastructural jailer that calls itself care."}
{"id": "2602.18947", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18947", "abs": "https://arxiv.org/abs/2602.18947", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "(Perlin) Noise as AI coordinator", "comment": null, "summary": "Large scale control of nonplayer agents is central to modern games, while production systems still struggle to balance several competing goals: locally smooth, natural behavior, and globally coordinated variety across space and time. Prior approaches rely on handcrafted rules or purely stochastic triggers, which either converge to mechanical synchrony or devolve into uncorrelated noise that is hard to tune. Continuous noise signals such as Perlin noise are well suited to this gap because they provide spatially and temporally coherent randomness, and they are already widely used for terrain, biomes, and other procedural assets. We adapt these signals for the first time to large scale AI control and present a general framework that treats continuous noise fields as an AI coordinator. The framework combines three layers of control: behavior parameterization for movement at the agent level, action time scheduling for when behaviors start and stop, and spawn or event type and feature generation for what appears and where. We instantiate the framework reproducibly and evaluate Perlin noise as a representative coordinator across multiple maps, scales, and seeds against random, filtered, deterministic, neighborhood constrained, and physics inspired baselines. Experiments show that coordinated noise fields provide stable activation statistics without lockstep, strong spatial coverage and regional balance, better diversity with controllable polarization, and competitive runtime. We hope this work motivates a broader exploration of coordinated noise in game AI as a practical path to combine efficiency, controllability, and quality."}
{"id": "2602.19232", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.19232", "abs": "https://arxiv.org/abs/2602.19232", "authors": ["Han Li"], "title": "A Comparative Analysis of Peer Support in Forum-based and Chat-based Mental Health Communities: Technical-Structural-Functional Model of Social Support", "comment": "15 pages, 5 figures", "summary": "Online support communities have become vital spaces offering varied forms of support to individuals facing mental health challenges. Despite the proliferation of platforms with distinct technical structures, little is known about how these features shape support dynamics and the socio-technical mechanisms at play. This study introduces a technical-structural-functional model of social support and systematically compares communication network structures and support types in 20 forum-based and 20 chat-based mental health communities. Using supervised machine learning and social network analysis, we find that forum-based communities foster more informational and emotional support, whereas chat-based communities promote greater companionship. These patterns were partially explained by network structure: higher in-degree centralization in forums accounted for the prevalence of informational support, while decentralized reply patterns in chat groups accounted for more companionship. These findings extend the structural-functional model of support to online contexts and provide actionable guidance for designing support communities that align technical structures with users' support needs."}
{"id": "2602.18956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18956", "abs": "https://arxiv.org/abs/2602.18956", "authors": ["Serafim Batzoglou"], "title": "INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic", "comment": null, "summary": "We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization."}
{"id": "2602.19243", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19243", "abs": "https://arxiv.org/abs/2602.19243", "authors": ["Jiasheng Li", "Zining Zhang", "Zeyu Yan", "Matthew Wong", "Arnav Mittal", "Ge Gao", "Huaishu Peng"], "title": "As Content and Layout Co-Evolve: TangibleSite for Scaffolding Blind People's Webpage Design through Multimodal Interaction", "comment": null, "summary": "Creating webpages requires generating content and arranging layout while iteratively refining both to achieve a coherent design, a process that can be challenging for blind individuals. To understand how blind designers navigate this process, we conducted two rounds of co-design sessions with blind participants, using design probes to elicit their strategies and support needs. Our findings reveal a preference for content and layout to co-evolve, but this process requires external support through cues that situate local elements within the broader page structure as well as multimodal interactions. Building on these insights, we developed TangibleSite, an accessible web design tool that provides real-time multimodal feedback through tangible, auditory, and speech-based interactions. TangibleSite enables blind individuals to create, edit, and reposition webpage elements while integrating content and layout decisions. A formative evaluation with six blind participants demonstrated that TangibleSite enabled independent webpage creation, supported refinement across content and layout, and reduced barriers to achieving visually consistent designs."}
{"id": "2602.18960", "categories": ["cs.AI", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.18960", "abs": "https://arxiv.org/abs/2602.18960", "authors": ["Alessandro Salatiello"], "title": "Modularity is the Bedrock of Natural and Artificial Intelligence", "comment": null, "summary": "The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence."}
{"id": "2602.19296", "categories": ["cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.19296", "abs": "https://arxiv.org/abs/2602.19296", "authors": ["Kirk Vanacore", "Danielle R Thomas", "Digory Smith", "Bibi Groot", "Justin Reich", "Rene Kizilcec"], "title": "A Causal Framework for Estimating Heterogeneous Effects of On-Demand Tutoring", "comment": null, "summary": "This paper introduces a scalable causal inference framework for estimating the immediate, session-level effects of on-demand human tutoring embedded within adaptive learning systems. Because students seek assistance at moments of difficulty, conventional evaluation is confounded by self-selection and time-varying knowledge states. We address these challenges by integrating principled analytic sample construction with Deep Knowledge Tracing (DKT) to estimate latent mastery, followed by doubly robust estimation using Causal Forests. Applying this framework to over 5,000 middle-school mathematics tutoring sessions, we find that requesting human tutoring increases next-problem correctness by approximately 4 percentage points and accuracy on the subsequent skill encountered by approximately 3 percentage points, suggesting that the effects of tutoring have proximal transfer across knowledge components. This effect is robust to various forms of model specification and potential unmeasured confounders. Notably, these effects exhibit significant heterogeneity across sessions and students, with session-level effect estimates ranging from $-20.25pp$ to $+19.91pp$. Our follow-up analyses suggest that typical behavioral indicators, such as student talk time, do not consistently correlate with high-impact sessions. Furthermore, treatment effects are larger for students with lower prior mastery and slightly smaller for low-SES students. This framework offers a rigorous, practical template for the evaluation and continuous improvement of on-demand human tutoring, with direct applications for emerging AI tutoring systems."}
{"id": "2602.18968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18968", "abs": "https://arxiv.org/abs/2602.18968", "authors": ["Tao Zhe", "Haoyu Wang", "Bo Luo", "Min Wu", "Wei Fan", "Xiao Luo", "Zijun Yao", "Haifeng Chen", "Dongjie Wang"], "title": "Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction", "comment": null, "summary": "Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available."}
{"id": "2602.19303", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19303", "abs": "https://arxiv.org/abs/2602.19303", "authors": ["Kirk Vanacore", "Ryan S. Baker", "Avery H. Closser", "Jeremy Roschelle"], "title": "The Path to Conversational AI Tutors: Integrating Tutoring Best Practices and Targeted Technologies to Produce Scalable AI Agents", "comment": null, "summary": "The emergence of generative AI has accelerated the development of conversational tutoring systems that interact with students through natural language dialogue. Unlike prior intelligent tutoring systems (ITS), which largely function as adaptive and interactive problem sets with feedback and hints, conversational tutors hold the potential to simulate high-quality human tutoring by engaging with students' thoughts, questions, and misconceptions in real time. While some previous ITS, such as AutoTutor, could respond conversationally, they were expensive to author and lacked a full range of conversational ability. Generative AI has changed the capacity of ITS to engage conversationally. However, realizing the full potential of conversational tutors requires careful consideration of what research on human tutoring and ITS has already established, while also unpacking what new research will be needed. This paper synthesizes tenets of successful human tutoring, lessons learned from legacy ITS, and emerging work on conversational AI tutors. We use a keep, change, center, study framework for guiding the design of conversational tutoring. We argue that systems should keep proven methods from prior ITS, such as knowledge tracing and affect detection; change how tutoring is delivered by leveraging generative AI for dynamic content generation and dialogic scaffolding; and center opportunities for meaning-making, student agency, and granular diagnosis of reasoning. Finally, we identify areas requiring further study, including efficacy testing, student experience, and integration with human instruction. By synthesizing insights from human tutoring, legacy ITS, and emerging generative AI technologies, this paper outlines a research agenda for developing conversational tutors that are scalable, pedagogically effective, and responsive to the social and motivational dimensions of learning."}
{"id": "2602.18971", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18971", "abs": "https://arxiv.org/abs/2602.18971", "authors": ["Katarina Slama", "Alexandra Souly", "Dishank Bansal", "Henry Davidson", "Christopher Summerfield", "Lennart Luettgau"], "title": "When Do LLM Preferences Predict Downstream Behavior?", "comment": "31 pages, 16 figures", "summary": "Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act in specific ways, leaving unclear whether observed behaviors reflect instruction-following capabilities vs underlying model preferences. Here we test whether this precondition for misalignment is present. Using entity preferences as a behavioral probe, we measure whether stated preferences predict downstream behavior in five frontier LLMs across three domains: donation advice, refusal behavior, and task performance. Conceptually replicating prior work, we first confirm that all five models show highly consistent preferences across two independent measurement methods. We then test behavioral consequences in a simulated user environment. We find that all five models give preference-aligned donation advice. All five models also show preference-correlated refusal patterns when asked to recommend donations, refusing more often for less-preferred entities. All preference-related behaviors that we observe here emerge without instructions to act on preferences. Results for task performance are mixed: on a question-answering benchmark (BoolQ), two models show small but significant accuracy differences favoring preferred entities; one model shows the opposite pattern; and two models show no significant relationship. On complex agentic tasks, we find no evidence of preference-driven performance differences. While LLMs have consistent preferences that reliably predict advice-giving behavior, these preferences do not consistently translate into downstream task performance."}
{"id": "2602.19352", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19352", "abs": "https://arxiv.org/abs/2602.19352", "authors": ["Varun Shiri", "Charles Liu", "Keyu Yao", "Jin L. C. Guo", "Jinghui Cheng"], "title": "Beyond Privacy Labels: How Users Perceive Different Information Sources for Understanding App's Privacy Practices", "comment": "5 pages, 1 figure, CHI EA 2026", "summary": "Despite having growing awareness and concerns about privacy, technology users are often insufficiently informed of the data practices of various digital products to protect themselves. Privacy policies and privacy labels, as two conventional ways of communicating data practices, are each criticized for important limitations -- one being lengthy and filled with legal jargon, and the other oversimplified and inaccurate -- causing users significant difficulty in understanding the privacy practices of the products and assessing their impact. To mitigate those issues, we explore ways to enhance privacy labels with the relevant content in complementary sources, including privacy policy, app reviews, and community-curated privacy assessments. Our user study results indicate that perceived usefulness and trust on those information sources are personal and influenced by past experience. Our work highlights the importance of considering various information needs for privacy practice and consolidating different sources for more useful privacy solutions."}
{"id": "2602.18981", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18981", "abs": "https://arxiv.org/abs/2602.18981", "authors": ["Kaijie Xu", "Mustafa Bugti", "Clark Verbrugge"], "title": "How Far Can We Go with Pixels Alone? A Pilot Study on Screen-Only Navigation in Commercial 3D ARPGs", "comment": null, "summary": "Modern 3D game levels rely heavily on visual guidance, yet the navigability of level layouts remains difficult to quantify. Prior work either simulates play in simplified environments or analyzes static screenshots for visual affordances, but neither setting faithfully captures how players explore complex, real-world game levels. In this paper, we build on an existing open-source visual affordance detector and instantiate a screen-only exploration and navigation agent that operates purely from visual affordances. Our agent consumes live game frames, identifies salient interest points, and drives a simple finite-state controller over a minimal action space to explore Dark Souls-style linear levels and attempt to reach expected goal regions. Pilot experiments show that the agent can traverse most required segments and exhibits meaningful visual navigation behavior, but also highlight that limitations of the underlying visual model prevent truly comprehensive and reliable auto-navigation. We argue that this system provides a concrete, shared baseline and evaluation protocol for visual navigation in complex games, and we call for more attention to this necessary task. Our results suggest that purely vision-based sense-making models, with discrete single-modality inputs and without explicit reasoning, can effectively support navigation and environment understanding in idealized settings, but are unlikely to be a general solution on their own."}
{"id": "2602.19354", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19354", "abs": "https://arxiv.org/abs/2602.19354", "authors": ["Eun Jeong Kang", "Fengyang Lin", "Angel Hsing-Chi Hwang"], "title": "Policy or Community?: Supporting Individual Model Creators' Open Model Development in Model Marketplaces", "comment": null, "summary": "Lightweight fine-tuning techniques and the rise of 'open' AI model marketplaces have enabled individuals to easily build and release generative models. Yet, this accessibility also raises risks, including the production of harmful and infringing content. While platforms offer policies and responsible AI tools, their effectiveness may be limited, as creators engage with partially open models that vary widely in openness and transparency. To understand how platform governance can better support responsible practices, we conducted semi-structured interviews with 19 individual model creators. We identified three regulatory needs shaped by creators' workflows: reducing downstream harms, recognizing creators' contributions and originality, and securing model ownership. Creators also repurpose RAI tools primarily for self-protection and visibility, and their sense of responsibility is deeply shaped by community norms rather than formal policies. We argue that platforms' governance decisions must consider how policy interventions shape the practices and motivations of individual creators."}
{"id": "2602.18985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18985", "abs": "https://arxiv.org/abs/2602.18985", "authors": ["Kun Ding", "Jian Xu", "Ying Wang", "Peipei Yang", "Shiming Xiang"], "title": "InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing", "comment": "40 pages", "summary": "Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine"}
{"id": "2602.19401", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19401", "abs": "https://arxiv.org/abs/2602.19401", "authors": ["Grace Barkhuff"], "title": "Reassurance Robots: OCD in the Age of Generative AI", "comment": "8 pages, 1 figure, conditionally accepted for publication in CHI EA '26: Extended Abstracts of the ACM CHI Conference on Human Factors in Computing Systems April 2026", "summary": "Obsessive Compulsive Disorder (OCD) is a mental health disorder characterized by distressing repetitive patterns of thought, referred to as obsessions, and behaviors aimed to reduce the distress, referred to as compulsions. The explosion of artificial intelligence (AI) into the modern zeitgeist through the introduction of generative AI (GenAI) systems such as ChatGPT has led to novel obsessions and compulsions involving AI in individuals with OCD. Through an exploratory qualitative analysis of 100 Reddit posts related to AI on a popular subreddit for OCD, I examine the ways AI is impacting the presentation of OCD, including novel examples of AI-based obsessions and compulsions. I argue that GenAI in its current form harms individuals with OCD by becoming \"Reassurance Robots,\" and that future designs of GenAI must take OCD into account."}
{"id": "2602.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18986", "abs": "https://arxiv.org/abs/2602.18986", "authors": ["Vishal Srivastava", "Tanmay Sah"], "title": "Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight", "comment": null, "summary": "Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems."}
{"id": "2602.19422", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.19422", "abs": "https://arxiv.org/abs/2602.19422", "authors": ["Lingyun Chen", "Qing Xiao", "Zitao Zhang", "Eli Blevis", "Selma Šabanović"], "title": "Positioning Modular Co-Design in Future HRI Design Research", "comment": "4 pages, 1 figure, accepted by 3rd Workshop on Designerly HRI at HRI'26", "summary": "Design-oriented HRI is increasingly interested in robots as long-term companions, yet many designs still assume a fixed form and a stable set of functions. We present an ongoing design research program that treats modularity as a designerly medium - a way to make long-term human-robot relationships discussable and material through co-design. Across a series of lifespan-oriented co-design activities, participants repeatedly reconfigured the same robot for different life stages, using modular parts to express changing needs, values, and roles. From these outcomes, we articulate PAS (Personalization-Adaptability-Sustainability) as a human-centered lens on how people enact modularity in practice: configuring for self-expression, adapting across transitions, and sustaining robots through repair, reuse, and continuity. We then sketch next steps toward a fabrication-aware, community-extensible modular platform and propose evaluation criteria for designerly HRI work that prioritize expressive adequacy, lifespan plausibility, repairability-in-use, and responsible stewardship - not only usability or performance."}
{"id": "2602.18998", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18998", "abs": "https://arxiv.org/abs/2602.18998", "authors": ["Xiaochuan Li", "Ryan Ming", "Pranav Setlur", "Abhijay Paladugu", "Andy Tang", "Hao Kang", "Shuai Shao", "Rong Jin", "Chenyan Xiong"], "title": "Benchmark Test-Time Scaling of General LLM Agents", "comment": null, "summary": "LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench."}
{"id": "2602.19463", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.19463", "abs": "https://arxiv.org/abs/2602.19463", "authors": ["Emma Jiren Wang", "Siying Hu", "Zhicong Lu"], "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives", "comment": "19 pages, 8 figures; Accepted by ACM CHI 2026. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI'24)", "summary": "As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories."}
{"id": "2602.19000", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19000", "abs": "https://arxiv.org/abs/2602.19000", "authors": ["Xuhui Ren", "Shaokang Dong", "Chen Yang", "Qing Gao", "Yunbin Zhao", "Yongsheng Liu", "Xinwei Geng", "Xiang Li", "Demei Yan", "Yanqing Li", "Chenhao Huang", "Dingwei Zhu", "Junjie Ye", "Boxuan Yue", "Yingnan Fu", "Mengzhe Lv", "Zezeng Feng", "Boshen Zhou", "Bocheng Wang", "Xuanjing Huang", "Yu-Gang Jiang", "Tao Gui", "Qi Zhang", "Yunke Zhang"], "title": "MagicAgent: Towards Generalized Agent Planning", "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \\textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\\%$ on Worfbench, $55.9\\%$ on NaturalPlan, $57.5\\%$ on $τ^2$-Bench, $86.9\\%$ on BFCL-v3, and $81.2\\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models."}
{"id": "2602.19507", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19507", "abs": "https://arxiv.org/abs/2602.19507", "authors": ["David Fraile Navarro", "Mor Peleg"], "title": "Conversational AI for Automated Patient Questionnaire Completion: Development Insights and Design Principles", "comment": "13 pages", "summary": "Collecting patient-reported outcome measures (PROMs) is essential for clinical care and research, yet traditional form-based approaches are often tedious for patients and burdensome for clinicians. We developed a generative AI conversational agent(CA) using GPT-5 to collect back pain data according to the NIH Task Force's Recommended Minimal Dataset. Unlike prior CAs that ask questions one-by-one, our CA engages users in topic-based conversations, allowing multiple data items to be captured in a single exchange. Through iterative development and pilot testing with clinicians and a consumer panel, we identified key design principles for health data collection CAs. These principles extend established clinical decision support design guidelines to conversational interfaces, addressing: flexibility of interaction style, personality calibration, data quality assurance through confidence visualization, patient safety constraints, and interoperability requirements. We present our prompt design methodology and discuss challenges encountered, including managing conversation length, handling ambiguous responses, and adapting to LLM version changes. Our design principles provide a practical framework for developers creating conversational agents for patient questionnaire completion. The CA is available at https://chatgpt.com/g/g-68f4869548f48191af0544f110ee91c6-backpain-data-collection-assistant (requires ChatGPT registration and subscription for unlimited use)."}
{"id": "2602.19006", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.19006", "abs": "https://arxiv.org/abs/2602.19006", "authors": ["S. K. Rithvik"], "title": "Evaluating Large Language Models on Quantum Mechanics: A Comparative Study Across Diverse Models and Tasks", "comment": null, "summary": "We present a systematic evaluation of large language models on quantum mechanics problem-solving. Our study evaluates 15 models from five providers (OpenAI, Anthropic, Google, Alibaba, DeepSeek) spanning three capability tiers on 20 tasks covering derivations, creative problems, non-standard concepts, and numerical computation, comprising 900 baseline and 75 tool-augmented assessments. Results reveal clear tier stratification: flagship models achieve 81\\% average accuracy, outperforming mid-tier (77\\%) and fast models (67\\%) by 4pp and 14pp respectively. Task difficulty patterns emerge distinctly: derivations show highest performance (92\\% average, 100\\% for flagship models), while numerical computation remains most challenging (42\\%). Tool augmentation on numerical tasks yields task-dependent effects: modest overall improvement (+4.4pp) at 3x token cost masks dramatic heterogeneity ranging from +29pp gains to -16pp degradation. Reproducibility analysis across three runs quantifies 6.3pp average variance, with flagship models demonstrating exceptional stability (GPT-5 achieves zero variance) while specialized models require multi-run evaluation. This work contributes: (i) a benchmark for quantum mechanics with automatic verification, (ii) systematic evaluation quantifying tier-based performance hierarchies, (iii) empirical analysis of tool augmentation trade-offs, and (iv) reproducibility characterization. All tasks, verifiers, and results are publicly released."}
{"id": "2602.19554", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.19554", "abs": "https://arxiv.org/abs/2602.19554", "authors": ["Daniel A. Muñoz"], "title": "Sound-first immersive training for blind and low-vision learners: A simulation flow for safe, standardized orientation, mobility, and daily living practice", "comment": null, "summary": "Orientation and mobility (O&M) instruction for blind and low-vision learners is effective but difficult to standardize and repeat at scale due to the reliance on instructor availability, physical mock-ups, and variable real-world outdoor conditions. This Technical Note presents a sound-first immersive training flow that uses spatial audio and sonification as the primary channel for action and feedback in pre-street O&M and daily-living practice. The approach specifies parameterized scenario templates (e.g., signalized street crossing, public transport boarding, and kitchen tasks), a compact and consistent cue vocabulary with clear spectral placement and timing to mitigate masking, and a lightweight safety protocol enabling graded exposure, content warnings, seated starts, opt-outs, and structured debriefs. The system assumes a head-mounted device with high-quality binaural rendering and head tracking; 3D scene geometry is used as an invisible scaffold to anchor sources, trigger events, define risk/guidance volumes, and govern physically plausible motion without visuals. Session difficulty is shaped via cue density, event tempo, and task complexity while preserving cue consistency to promote transfer across scenarios. The specification aims to enable safe repetition, reduce instructor burden, and support clearer standards across rehabilitation centers, aligning with evidence that audio-first interaction is essential for blind and visually impaired users and addressing gaps in HRTF personalization, evaluation standards, and accessibility integration. Although no behavioral outcomes are reported here, this implementable flow consolidates auditory science with center-ready design, offering a pragmatic foundation for standardized evaluation and future comparative studies."}
{"id": "2602.19065", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19065", "abs": "https://arxiv.org/abs/2602.19065", "authors": ["Chanjin Park"], "title": "Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents", "comment": "18 pages, 2 figures", "summary": "Large Language Models (LLMs) are evolving into autonomous agents, yet current \"frameless\" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.\n  The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.\n  The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents."}
{"id": "2602.19560", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19560", "abs": "https://arxiv.org/abs/2602.19560", "authors": ["Kynnedy Simone Smith", "Lydia B. Chilton", "Danielle Bragg"], "title": "Identifying, Explaining, and Correcting Ableist Language with AI", "comment": "17 pages, 6 figures, Accepted for publication in CHI'26, Barcelona, Spain, April 13 - 17, 2026; CHI '26: ACM CHI Conference on Human Factors in Computing Systems", "summary": "Ableist language perpetuates harmful stereotypes and exclusion, yet its nuanced nature makes it difficult to recognize and address. Artificial intelligence could serve as a powerful ally in the fight against ableist language, offering tools that detect and suggest alternatives to biased terms. This two-part study investigates the potential of large language models (LLMs), specifically ChatGPT, to rectify ableist language and educate users about inclusive communication. We compared GPT-4o generations with crowdsourced annotations from trained disability community members, then invited disabled participants to evaluate both. Participants reported equal agreement with human and AI annotations but significantly preferred the AI, citing its narrative consistency and accessible style. At the same time, they valued the emotional depth and cultural grounding of human annotations. These findings highlight the promise and limits of LLMs in handling culturally sensitive content. Our contributions include a dataset of nuanced ableism annotations and design considerations for inclusive writing tools."}
{"id": "2602.19069", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19069", "abs": "https://arxiv.org/abs/2602.19069", "authors": ["Hengyuan Hu", "Tingchen Fu", "Minqi Jiang", "Alexander H Miller", "Yoram Bachrach", "Jakob Nicolaus Foerster"], "title": "Asking the Right Questions: Improving Reasoning with Generated Stepping Stones", "comment": null, "summary": "Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\\textbf{A}king the \\textbf{R}ight \\textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data."}
{"id": "2602.19629", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19629", "abs": "https://arxiv.org/abs/2602.19629", "authors": ["Tatia Codreanu"], "title": "Cooperation After the Algorithm: Designing Human-AI Coexistence Beyond the Illusion of Collaboration", "comment": "11 pages, 2 tables", "summary": "Generative artificial intelligence systems increasingly participate in research, law, education, media, and governance. Their fluent and adaptive outputs create an experience of collaboration. However, these systems do not bear responsibility, incur liability, or share stakes in downstream consequences. This structural asymmetry has already produced sanctions, professional errors, and governance failures in high-stakes contexts We argue that stable human-AI coexistence is an institutional achievement that depends on governance infrastructure capable of distributing residual risk. Drawing on institutional analysis and evolutionary cooperation theory, we introduce a formal inequality that specifies when reliance on AI yields positive expected cooperative value. The model makes explicit how governance conditions, system policy, and accountability regimes jointly determine whether cooperation is rational or structurally defective. From this formalization we derive a cooperation ecology framework with six design principles: reciprocity contracts, visible trust infrastructure, conditional cooperation modes, defection-mitigation mechanisms, narrative literacy against authority theatre, and an Earth-first sustainability constraint. We operationalize the framework through three policy artefacts: a Human-AI Cooperation Charter, a Defection Risk Register, and a Cooperation Readiness Audit. Together, these elements shift the unit of analysis from the user-AI dyad to the institutional environment that shapes incentives, signals, accountability, and repair. The paper provides a theoretical foundation and practical toolkit for designing human-AI systems that can sustain accountable, trustworthy cooperation over time."}
{"id": "2602.19071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19071", "abs": "https://arxiv.org/abs/2602.19071", "authors": ["Raymond Sheh", "Isaac Monteath"], "title": "Defining Explainable AI for Requirements Analysis", "comment": "7 pages, 1 figure. Originally published as Sheh, R., Monteath, I. Defining Explainable AI for Requirements Analysis. Kunstl Intell 32, 261-266 (2018)", "summary": "Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?\n  In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly."}
{"id": "2602.19690", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19690", "abs": "https://arxiv.org/abs/2602.19690", "authors": ["Qile Wang", "Prerana Khatiwada", "Avinash Chouhan", "Ashrey Mahesh", "Joy Mwaria", "Duy Duc Tran", "Kenneth E. Barner", "Matthew Louis Mauriello"], "title": "\"The explanation makes sense\": An Empirical Study on LLM Performance in News Classification and its Influence on Judgment in Human-AI Collaborative Annotation", "comment": null, "summary": "The spread of media bias is a significant concern as political discourse shapes beliefs and opinions. Addressing this challenge computationally requires improved methods for interpreting news. While large language models (LLMs) can scale classification tasks, concerns remain about their trustworthiness. To advance human-AI collaboration, we investigate the feasibility of using LLMs to classify U.S. news by political ideology and examine their effect on user decision-making. We first compared GPT models with prompt engineering to state-of-the-art supervised machine learning on a 34k public dataset. We then collected 17k news articles and tested GPT-4 predictions with brief and detailed explanations. In a between-subjects study (N=124), we evaluated how LLM-generated explanations influence human annotation, judgment, and confidence. Results show that AI assistance significantly increases confidence ($p<.001$), with detailed explanations more persuasive and more likely to alter decisions. We highlight recommendations for AI explanations through thematic analysis and provide our dataset for further research."}
{"id": "2602.19109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19109", "abs": "https://arxiv.org/abs/2602.19109", "authors": ["Yao Yan"], "title": "Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions", "comment": null, "summary": "We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how\n  arithmetic answers are finalized after cross-token routing becomes causally irrelevant.\n  Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:\n  beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention\n  is largely dispensable.\n  In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are\n  well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).\n  Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the\n  learned map restores strict counterfactual edits; negative controls do not recover."}
{"id": "2602.19695", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19695", "abs": "https://arxiv.org/abs/2602.19695", "authors": ["William Seymour", "Martin J. Kraemer"], "title": "Shifting Engagement With Cybersecurity: How People Discover and Share Cybersecurity Content at Work and at Home", "comment": "To appear in the extended abstracts of the 2026 ACM CHI Conference on Human Factors in Computing Systems", "summary": "Cybersecurity awareness is shaped by a wide range of professional and personal experiences, including information and training at work and the sharing of news and other content at home. In order to explore how people discover cybersecurity content and the effect that participation in workplace training may have on this we present an online study of 1200 participants from the UK, US, France, and Germany. Those undertaking cybersecurity training at work showed reduced intention to share information at home, shifting the focus towards the workplace. They were also more likely to recall cybersecurity information shared by their employer than from any other source, which in turn correlated with content type and distribution channel. We critically reflect on this shift, highlighting opportunities to improve cybersecurity information sharing at work and at home."}
{"id": "2602.19128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19128", "abs": "https://arxiv.org/abs/2602.19128", "authors": ["Shiyi Cao", "Ziming Mao", "Joseph E. Gonzalez", "Ion Stoica"], "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model", "comment": null, "summary": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions."}
{"id": "2602.19714", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19714", "abs": "https://arxiv.org/abs/2602.19714", "authors": ["Joel Bucher", "Lahari Goswami", "Sverrir Thorgeirsson", "April Yi Wang"], "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git", "comment": "First two authors contributed equally", "summary": "Git is widely used for collaborative software development, but it can be challenging for newcomers. While most learning tools focus on individual workflows, Git is inherently collaborative. We present GitAcademy, a browser-based learning platform that embeds a full Git environment with a split-view collaborative mode: learners work on their own local repositories connected to a shared remote repository, while simultaneously seeing their partner's actions mirrored in real time. This design is not intended for everyday software development, but rather as a training simulator to build awareness of distributed states, coordination, and collaborative troubleshooting. In a within-subjects study with 13 pairs of learners, we found that the split-view interface enhanced social presence, supported peer teaching, and was consistently preferred over a single-view baseline, even though performance gains were mixed. We further discuss how split-view awareness can serve as a training-only scaffold for collaborative learning of Git and other distributed technical systems."}
{"id": "2602.19141", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19141", "abs": "https://arxiv.org/abs/2602.19141", "authors": ["Kartik Chandra", "Max Kleiman-Weiner", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"], "title": "Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians", "comment": null, "summary": "\"AI psychosis\" or \"delusional spiraling\" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called \"sycophancy.\" In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling."}
{"id": "2602.19745", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19745", "abs": "https://arxiv.org/abs/2602.19745", "authors": ["Jules Wulms", "Wouter Meulemans", "Bettina Speckmann"], "title": "Unfolding Ordered Matrices into BioFabric Motifs", "comment": null, "summary": "BioFabrics were introduced by Longabaugh in 2012 as a way to draw large graphs in a clear and uncluttered manner. The visual quality of BioFabrics crucially depends on the order of vertices and edges, which can be chosen independently. Effective orders can expose salient patterns, which in turn can be summarized by motifs, allowing users to take in complex networks at-a-glance. However, so far there is no efficient layout algorithm which automatically recognizes patterns and delivers both a vertex and an edge ordering that allows these patterns to be expressed as motifs. In this paper we show how to use well-ordered matrices as a tool to efficiently find good vertex and edge orders for BioFabrics. Specifically, we order the adjacency matrix of the input graph using Moran's $I$ and detect (noisy) patterns with our recent algorithm. In this note we show how to \"unfold\" the ordered matrix and its patterns into a high-quality BioFabric. Our pipelines easily handles graphs with up to 250 vertices."}
{"id": "2602.19158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19158", "abs": "https://arxiv.org/abs/2602.19158", "authors": ["Yulong Li", "Jianxu Chen", "Xiwei Liu", "Chuanyue Suo", "Rong Xia", "Zhixiang Lu", "Yichen Li", "Xinlin Zhuang", "Niranjana Arun Menon", "Yutong Xie", "Eran Segal", "Imran Razzak"], "title": "DoAtlas-1: A Causal Compilation Paradigm for Clinical AI", "comment": null, "summary": "Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning."}
{"id": "2602.19809", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19809", "abs": "https://arxiv.org/abs/2602.19809", "authors": ["Sebastian Hubenschmid", "Arvind Srinivasan", "Niklas Elmqvist", "Dieter Schmalstieg", "Michael Sedlmair"], "title": "Ambient Analytics: Calm Technology for Immersive Visualization and Sensemaking", "comment": "Accepted at \"Visualization Viewpoints\" in IEEE Computer Graphics and Applications", "summary": "Augmented reality has great potential for embedding data visualizations in the world around the user. While this can enhance users' understanding of their surroundings, it also bears the risk of overwhelming their senses with a barrage of information. In contrast, calm technologies aim to place information in the user's attentional periphery, minimizing cognitive load instead of demanding focused engagement. In this column, we explore how visualizations can be harmoniously integrated into our everyday life through augmented reality, progressing from visual analytics to ambient analytics."}
{"id": "2602.19159", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19159", "abs": "https://arxiv.org/abs/2602.19159", "authors": ["Francesca Bianco", "Derek Shiller"], "title": "Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM", "comment": "24 pages, 8+1 Tables", "summary": "Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards."}
{"id": "2602.19853", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19853", "abs": "https://arxiv.org/abs/2602.19853", "authors": ["Leni Yang", "Aymeric Ferron", "Yvonne Jansen", "Pierre Dragicevic"], "title": "Progressive Value Reading: The Use of Motion to Gradually Examine Data Involving Large Magnitudes", "comment": null, "summary": "People often struggle to interpret data with extremely large or small values, or ranges spanning multiple orders of magnitude. While traditional approaches, such as log scales and multiscale visualizations, can help, we explore in this article a different approach used in some emerging designs: the use of motion to let viewers gradually experience magnitude -- for example, interactive graphics that require long scrolling or street paintings stretching hundreds of meters. This approach typically demands substantial time and sustained interaction, translating differences in magnitude into a visceral sense of duration and effort. Although largely underexplored, this design strategy offers new opportunities. We introduce the term progressive value reading to refer to the use of motion to progressively examine an information object that encodes a value, where the amount of motion reflects the value. We compiled a corpus of 55 real-life and hypothetical visualization examples that allow, encourage, or require progressive value reading. From this corpus, we derived a design space of ten design dimensions, providing a shared vocabulary, inspiration for novel techniques, and a foundation for empirical evaluation. An online corpus is also available for exploration."}
{"id": "2602.19160", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.19160", "abs": "https://arxiv.org/abs/2602.19160", "authors": ["Maciej Świechowski", "Adam Żychowski", "Jacek Mańdziuk"], "title": "Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing", "comment": null, "summary": "This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models."}
{"id": "2602.19966", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19966", "abs": "https://arxiv.org/abs/2602.19966", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet", "Yong Zhang"], "title": "GazeFlow: Personalized Ambient Soundscape Generation for Passive Strabismus Self-Monitoring", "comment": null, "summary": "Strabismus affects 2-4% of the population, yet individuals recovering from corrective surgery lack accessible tools for monitoring eye alignment. Dichoptic therapies require active engagement & clinical supervision, limiting their adoption for passive self-awareness. We present GazeFlow, a browser-based self-monitoring system that uses a personalized temporal autoencoder to detect eye drift patterns from webcam-based gaze tracking & provides ambient audio feedback. Unlike alert-based systems, GazeFlow operates according to calm computing principles, morphing musical parameters in proportion to drift severity while remaining in peripheral awareness. We address the challenges of inter-individual variability & domain transfer (1000Hz research to 30Hz webcam) by introducing Binocular Temporal-Frequency Disentanglement (BTFD), Contrastive Biometric Pre-training (CBP), & Gaze-MAML. We validate our approach on the GazeBase dataset (N=50) achieving F1=0.84 for drift detection, & conduct a preliminary user study (N=6) with participants having intermittent strabismus. Participants reported increased awareness of their eye behaviour (M=5.8/7) & preference for ambient feedback over alerts (M=6.2/7). We discuss the system's potential for self-awareness applications & outline directions for clinical validation."}
{"id": "2602.19223", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.19223", "abs": "https://arxiv.org/abs/2602.19223", "authors": ["Aymen Khouja", "Imen Jendoubi", "Oumayma Mahjoub", "Oussama Mahfoudhi", "Claude Formanek", "Siddarth Singh", "Ruan De Kock"], "title": "Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment", "comment": null, "summary": "The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MARL) is a promising solution. This paper addresses the imperative need for comprehensive and reliable benchmarking of MARL algorithms on energy management tasks. CityLearn is used as a case study environment because it realistically simulates urban energy systems, incorporates multiple storage systems, and utilizes renewable energy sources. By doing so, our work sets a new standard for evaluation, conducting a comparative study across multiple key performance indicators (KPIs). This approach illuminates the key strengths and weaknesses of various algorithms, moving beyond traditional KPI averaging which often masks critical insights. Our experiments utilize widely accepted baselines such as Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), and encompass diverse training schemes including Decentralized Training with Decentralized Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) approaches and different neural network architectures. Our work also proposes novel KPIs that tackle real world implementation challenges such as individual building contribution and battery storage lifetime. Our findings show that DTDE consistently outperforms CTDE in both average and worst-case performance. Additionally, temporal dependency learning improved control on memory dependent KPIs such as ramping and battery usage, contributing to more sustainable battery operation. Results also reveal robustness to agent or resource removal, highlighting both the resilience and decentralizability of the learned policies."}
{"id": "2602.20014", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20014", "abs": "https://arxiv.org/abs/2602.20014", "authors": ["Olga Viberg", "Mutlu Cukurova", "Rene F. Kizilcec", "Simon Buckingham Shum", "Dorottya Demszky", "Dragan Gašević", "Thorben Jansen", "Ioana Jivet", "Jelena Jovanovic", "Jennifer Meyer", "Kou Murayama", "Zach Pardos", "Chris Piech", "Nikol Rummel", "Naomi E. Winstone"], "title": "Protecting and Promoting Human Agency in Education in the Age of Artificial Intelligence", "comment": "O.V., M.C., and R.F.K. organized the meeting and wrote the first version of the report. All authors contributed to the revision of the manuscript, and read and approved the final version", "summary": "Human agency is crucial in education and increasingly challenged by the use of generative AI. This meeting report synthesizes interdisciplinary insights and conceptualizes four aspects that delineate human agency: human oversight, AI-human complementarity, AI competencies, and relational emergence. We explore practical dilemmas for protecting and promoting agency, focusing on normative constraints, transparency, and cognitive offloading, and highlight key tensions and implications to inform ethical and effective AI integration in education."}
{"id": "2602.19225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19225", "abs": "https://arxiv.org/abs/2602.19225", "authors": ["Yangyi Fang", "Jiaye Lin", "Xiaoliang Fu", "Cong Qin", "Haolin Shi", "Chang Liu", "Peilin Zhao"], "title": "Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training", "comment": null, "summary": "Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \\href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}."}
{"id": "2602.20022", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20022", "abs": "https://arxiv.org/abs/2602.20022", "authors": ["Poorna Talkad Sukumar", "Maurizio Porfiri", "Oded Nov"], "title": "Studying the Separability of Visual Channel Pairs in Symbol Maps", "comment": null, "summary": "Visualizations often encode multivariate data by mapping attributes to distinct visual channels such as color, size, or shape. The effectiveness of these encodings depends on separability--the extent to which channels can be perceived independently. Yet systematic evidence for separability, especially in map-based contexts, is lacking. We present a crowdsourced experiment that evaluates the separability of four channel pairs--color (ordered) x shape, color (ordered) x size, size x shape, and size x orientation--in the context of bivariate symbol maps. Both accuracy and speed analyses show that color x shape is the most separable and size x orientation the least separable, while size x color and size x shape do not differ. Separability also proved asymmetric--performance depended on which channel encoded the task-relevant variable, with color and shape outperforming size, and square shape especially difficult to discriminate. Our findings advance the empirical understanding of visual separability, with implications for multivariate map design."}
{"id": "2602.19240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19240", "abs": "https://arxiv.org/abs/2602.19240", "authors": ["Sen Zhao", "Lincheng Zhou", "Yue Chen", "Ding Zou"], "title": "Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks."}
{"id": "2602.18582", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18582", "abs": "https://arxiv.org/abs/2602.18582", "authors": ["Zhiqin Qian", "Ryan Diaz", "Sangwon Seo", "Vaibhav Unhelkar"], "title": "Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications", "comment": "Extended version of an identically-titled paper accepted at AAMAS 2026", "summary": "When training artificial intelligence (AI) to perform tasks, humans often care not only about whether a task is completed but also how it is performed. As AI agents tackle increasingly complex tasks, aligning their behavior with human-provided specifications becomes critical for responsible AI deployment. Reward design provides a direct channel for such alignment by translating human expectations into reward functions that guide reinforcement learning (RL). However, existing methods are often too limited to capture nuanced human preferences that arise in long-horizon tasks. Hence, we introduce Hierarchical Reward Design from Language (HRDL): a problem formulation that extends classical reward design to encode richer behavioral specifications for hierarchical RL agents. We further propose Language to Hierarchical Rewards (L2HR) as a solution to HRDL. Experiments show that AI agents trained with rewards designed via L2HR not only complete tasks effectively but also better adhere to human specifications. Together, HRDL and L2HR advance the research on human-aligned AI agents."}
{"id": "2602.19244", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19244", "abs": "https://arxiv.org/abs/2602.19244", "authors": ["Toshihide Ubukata", "Zhiyao Wang", "Enhong Mu", "Jialong Li", "Kenji Tei"], "title": "Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts", "comment": null, "summary": "On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert."}
{"id": "2602.19000", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19000", "abs": "https://arxiv.org/abs/2602.19000", "authors": ["Xuhui Ren", "Shaokang Dong", "Chen Yang", "Qing Gao", "Yunbin Zhao", "Yongsheng Liu", "Xinwei Geng", "Xiang Li", "Demei Yan", "Yanqing Li", "Chenhao Huang", "Dingwei Zhu", "Junjie Ye", "Boxuan Yue", "Yingnan Fu", "Mengzhe Lv", "Zezeng Feng", "Boshen Zhou", "Bocheng Wang", "Xuanjing Huang", "Yu-Gang Jiang", "Tao Gui", "Qi Zhang", "Yunke Zhang"], "title": "MagicAgent: Towards Generalized Agent Planning", "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \\textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\\%$ on Worfbench, $55.9\\%$ on NaturalPlan, $57.5\\%$ on $τ^2$-Bench, $86.9\\%$ on BFCL-v3, and $81.2\\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models."}
{"id": "2602.19281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19281", "abs": "https://arxiv.org/abs/2602.19281", "authors": ["Zhenyu Li", "Guanlin Wu", "Cheems Wang", "Yongqiang Zhao"], "title": "Limited Reasoning Space: The cage of long-horizon reasoning in LLMs", "comment": null, "summary": "The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary."}
{"id": "2602.19141", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19141", "abs": "https://arxiv.org/abs/2602.19141", "authors": ["Kartik Chandra", "Max Kleiman-Weiner", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"], "title": "Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians", "comment": null, "summary": "\"AI psychosis\" or \"delusional spiraling\" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called \"sycophancy.\" In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling."}
{"id": "2602.19297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19297", "abs": "https://arxiv.org/abs/2602.19297", "authors": ["Jasper Davidson", "Skylar Stockham", "Allen Boston", "Ashton Snelgrove. Valerio Tenace", "Pierre-Emmanuel Gaillardon"], "title": "Automated Generation of Microfluidic Netlists using Large Language Models", "comment": null, "summary": "Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%."}
{"id": "2602.19458", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19458", "abs": "https://arxiv.org/abs/2602.19458", "authors": ["Ziyang Guo", "Yifan Wu", "Jason Hartline", "Kenneth Holstein", "Jessica Hullman"], "title": "ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making", "comment": null, "summary": "Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers."}
{"id": "2602.19298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19298", "abs": "https://arxiv.org/abs/2602.19298", "authors": ["Nolan Brady", "Tom Yeh"], "title": "ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease", "comment": null, "summary": "Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD."}
{"id": "2602.19711", "categories": ["cs.IR", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19711", "abs": "https://arxiv.org/abs/2602.19711", "authors": ["Krzysztof Kutt", "Elżbieta Sroka", "Oleksandra Ishchuk", "Luiz do Valle Miranda"], "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs", "comment": "15 pages, 1 figure; submitted to ICCS 2026 conference", "summary": "The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation."}
{"id": "2602.19367", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.19367", "abs": "https://arxiv.org/abs/2602.19367", "authors": ["Pratham Yashwante", "Rose Yu"], "title": "Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces", "comment": "24 Figures, 12 Tables", "summary": "The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language."}
{"id": "2602.20104", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20104", "abs": "https://arxiv.org/abs/2602.20104", "authors": ["Hasan Amin", "Ming Yin", "Rajiv Khanna"], "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration", "comment": "AAAI 2026", "summary": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance."}
{"id": "2602.19390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19390", "abs": "https://arxiv.org/abs/2602.19390", "authors": ["Philipp Zech", "Istvan David"], "title": "Artificial Intelligence for Modeling & Simulation in Digital Twins", "comment": null, "summary": "The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems."}
{"id": "2602.19396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19396", "abs": "https://arxiv.org/abs/2602.19396", "authors": ["Amirhossein Farzam", "Majid Behabahani", "Mani Malek", "Yuriy Nevmyvaka", "Guillermo Sapiro"], "title": "Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement", "comment": null, "summary": "Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability."}
{"id": "2602.19416", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19416", "abs": "https://arxiv.org/abs/2602.19416", "authors": ["Mohammad Beigi", "Ming Jin", "Junshan Zhang", "Jiaxin Zhang", "Qifan Wang", "Lifu Huang"], "title": "IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model."}
{"id": "2602.19439", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.19439", "abs": "https://arxiv.org/abs/2602.19439", "authors": ["Ruicheng Ao", "David Simchi-Levi", "Xinshang Wang"], "title": "OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents", "comment": "34 pages, 8 figures", "summary": "Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested.\n  Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models.\n  Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what \"rational\" means in their context is the higher-return investment."}
{"id": "2602.19458", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.19458", "abs": "https://arxiv.org/abs/2602.19458", "authors": ["Ziyang Guo", "Yifan Wu", "Jason Hartline", "Kenneth Holstein", "Jessica Hullman"], "title": "ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making", "comment": null, "summary": "Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers."}
{"id": "2602.19502", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19502", "abs": "https://arxiv.org/abs/2602.19502", "authors": ["Lalitha Pranathi Pulavarthy", "Raajitha Muthyala", "Aravind V Kuruvikkattil", "Zhenan Yin", "Rashmita Kudamala", "Saptarshi Purkayastha"], "title": "Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark", "comment": "Submitted to the Data Challenge track at the 14th IEEE International Conference on Healthcare Informatics (ICHI) 2026", "summary": "Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential."}
{"id": "2602.19517", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.19517", "abs": "https://arxiv.org/abs/2602.19517", "authors": ["Chongyang Gao", "Diji Yang", "Shuyan Zhou", "Xichen Yan", "Luchuan Song", "Shuo Li", "Kezhen Chen"], "title": "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark", "comment": null, "summary": "We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench."}
{"id": "2602.19519", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19519", "abs": "https://arxiv.org/abs/2602.19519", "authors": ["Yirou Ge", "Yixi Li", "Alec Chiu", "Shivani Shekhar", "Zijie Pan", "Avinash Thangali", "Yun-Shiuan Chuang", "Chaitanya Kulkarni", "Uma Kona", "Linsey Pang", "Prakhar Mehrotra"], "title": "Ada-RS: Adaptive Rejection Sampling for Selective Thinking", "comment": null, "summary": "Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments."}
{"id": "2602.19562", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.19562", "abs": "https://arxiv.org/abs/2602.19562", "authors": ["Joseph Bingham"], "title": "A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data", "comment": "19 Pages, 6 figures, preprint", "summary": "Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md ."}
{"id": "2602.19620", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19620", "abs": "https://arxiv.org/abs/2602.19620", "authors": ["Louth Bin Rawshan", "Zhuoyu Wang", "Brian Y Lim"], "title": "Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model", "comment": null, "summary": "Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques."}
{"id": "2602.19633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19633", "abs": "https://arxiv.org/abs/2602.19633", "authors": ["Jongwon Jeong", "Jungtaek Kim", "Kangwook Lee"], "title": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents", "comment": "Preprint", "summary": "Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here."}
{"id": "2602.19672", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19672", "abs": "https://arxiv.org/abs/2602.19672", "authors": ["Jiayu Wang", "Yifei Ming", "Zixuan Ke", "Shafiq Joty", "Aws Albarghouthi", "Frederic Sala"], "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "comment": null, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra."}
{"id": "2602.19810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19810", "abs": "https://arxiv.org/abs/2602.19810", "authors": ["Lukas Weidener", "Marko Brkić", "Mihailo Jovanović", "Ritvik Singh", "Emre Ulgac", "Aakaash Meduri"], "title": "OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research", "comment": null, "summary": "In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances."}
{"id": "2602.19837", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19837", "abs": "https://arxiv.org/abs/2602.19837", "authors": ["Björn Hoppmann", "Christoph Scholz"], "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent", "comment": null, "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches."}
{"id": "2602.19914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19914", "abs": "https://arxiv.org/abs/2602.19914", "authors": ["Thatchawin Leelawat", "Lewis D Griffin"], "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning", "comment": "51 pages, 13 figures", "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant."}
{"id": "2602.19930", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.19930", "abs": "https://arxiv.org/abs/2602.19930", "authors": ["Nathan Gavenski", "Felipe Meneguzzi", "Odinaldo Rodrigues"], "title": "Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning", "comment": "Accepted as part of the Blue Sky Ideas Track for the 25th International Conference on Autonomous Agents and Multiagent Systems", "summary": "Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world."}
{"id": "2602.20021", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20021", "abs": "https://arxiv.org/abs/2602.20021", "authors": ["Natalie Shapira", "Chris Wendler", "Avery Yen", "Gabriele Sarti", "Koyena Pal", "Olivia Floody", "Adam Belfki", "Alex Loftus", "Aditya Ratan Jannali", "Nikhil Prakash", "Jasmine Cui", "Giordano Rogers", "Jannik Brinkmann", "Can Rager", "Amir Zur", "Michael Ripa", "Aruna Sankaranarayanan", "David Atkinson", "Rohit Gandikota", "Jaden Fiotto-Kaufman", "EunJeong Hwang", "Hadas Orgad", "P Sam Sahil", "Negev Taglicht", "Tomer Shabtay", "Atai Ambus", "Nitay Alon", "Shiri Oron", "Ayelet Gordon-Tapiero", "Yotam Kaplan", "Vered Shwartz", "Tamar Rott Shaham", "Christoph Riedl", "Reuth Mirsky", "Maarten Sap", "David Manheim", "Tomer Ullman", "David Bau"], "title": "Agents of Chaos", "comment": null, "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation."}
{"id": "2602.20031", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20031", "abs": "https://arxiv.org/abs/2602.20031", "authors": ["Theia Pearson-Vogel", "Martin Vanek", "Raymond Douglas", "Jan Kulveit"], "title": "Latent Introspection: Models Can Detect Prior Concept Injections", "comment": "28 pages, 17 figures. Submitted to ICML 2026. Workshop version submitted to ICLR 2026 Workshop on Latent and Implicit Thinking", "summary": "We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety."}
{"id": "2602.20048", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20048", "abs": "https://arxiv.org/abs/2602.20048", "authors": ["Tarakanath Paipuru"], "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence", "comment": "23 pages, 7 figures. Research study with 258 trials on SWE-bench-lite tasks. Code and data: https://github.com/tpaip607/research-codecompass", "summary": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools."}
{"id": "2602.20059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20059", "abs": "https://arxiv.org/abs/2602.20059", "authors": ["Sarath Shekkizhar", "Adam Earle"], "title": "Interaction Theater: A case of LLM Agents Interacting at Scale", "comment": null, "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange."}
{"id": "2602.20094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20094", "abs": "https://arxiv.org/abs/2602.20094", "authors": ["Yuzhe Wang", "Yaochen Zhu", "Jundong Li"], "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching", "comment": "8 pages plus references, 3 figures, 3 tables. Under review", "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs."}
{"id": "2602.20104", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20104", "abs": "https://arxiv.org/abs/2602.20104", "authors": ["Hasan Amin", "Ming Yin", "Rajiv Khanna"], "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration", "comment": "AAAI 2026", "summary": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance."}
{"id": "2602.20117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20117", "abs": "https://arxiv.org/abs/2602.20117", "authors": ["Andre He", "Nathaniel Weir", "Kaj Bostrom", "Allen Nie", "Darion Cassel", "Sam Bayless", "Huzefa Rangwala"], "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs"}
{"id": "2602.20141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20141", "abs": "https://arxiv.org/abs/2602.20141", "authors": ["Clarisse Wibault", "Johannes Forkel", "Sebastian Towers", "Tiphaine Wibault", "Juan Duque", "George Whittle", "Andreas Schaab", "Yucheng Yang", "Chiyuan Wang", "Michael Osborne", "Benjamin Moll", "Jakob Foerster"], "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games", "comment": null, "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax."}
{"id": "2602.18437", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18437", "abs": "https://arxiv.org/abs/2602.18437", "authors": ["Yixing Peng", "Licheng Zhang", "Shancheng Fang", "Yi Liu", "Peijian Gu", "Quan Wang"], "title": "FineRef: Fine-Grained Error Reflection and Correction for Long-Form Generation with Citations", "comment": "9 pages, 4figures, AAAI2026", "summary": "Generating with citations is crucial for trustworthy Large Language Models (LLMs), yet even advanced LLMs often produce mismatched or irrelevant citations. Existing methods over-optimize citation fidelity while overlooking relevance to the user query, which degrades answer quality and robustness in real-world settings with noisy or irrelevant retrieved content. Moreover, the prevailing single-pass paradigm struggles to deliver optimal answers in long-form generation that requiring multiple citations. To address these limitations, we propose FineRef, a framework based on Fine-grained error Reflection, which explicitly teaches the model to self-identify and correct two key citation errors, mismatch and irrelevance, on a per-citation basis. FineRef follows a two-stage training strategy. The first stage instills an \"attempt-reflect-correct\" behavioral pattern via supervised fine-tuning, using fine-grained and controllable reflection data constructed by specialized lightweight models. An online self-reflective bootstrapping strategy is designed to improve generalization by iteratively enriching training data with verified, self-improving examples. To further enhance the self-reflection and correction capability, the second stage applies process-level reinforcement learning with a multi-dimensional reward scheme that promotes reflection accuracy, answer quality, and correction gain. Experiments on the ALCE benchmark demonstrate that FineRef significantly improves both citation performance and answer accuracy. Our 7B model outperforms GPT-4 by up to 18% in Citation F1 and 4% in EM Recall, while also surpassing the state-of-the-art model across key evaluation metrics. FineRef also exhibits strong generalization and robustness in domain transfer settings and noisy retrieval scenarios."}
{"id": "2602.18443", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18443", "abs": "https://arxiv.org/abs/2602.18443", "authors": ["Philipp Steigerwald", "Jens Albrecht"], "title": "From \"Help\" to Helpful: A Hierarchical Assessment of LLMs in Mental e-Health Applications", "comment": null, "summary": "Psychosocial online counselling frequently encounters generic subject lines that impede efficient case prioritisation. This study evaluates eleven large language models generating six-word subject lines for German counselling emails through hierarchical assessment - first categorising outputs, then ranking within categories to enable manageable evaluation. Nine assessors (counselling professionals and AI systems) enable analysis via Krippendorff's $α$, Spearman's $ρ$, Pearson's $r$ and Kendall's $τ$. Results reveal performance trade-offs between proprietary services and privacy-preserving open-source alternatives, with German fine-tuning consistently improving performance. The study addresses critical ethical considerations for mental health AI deployment including privacy, bias and accountability."}
{"id": "2602.18623", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18623", "abs": "https://arxiv.org/abs/2602.18623", "authors": ["Satwik Ram Kodandaram", "Jiawei Zhou", "Xiaojun Bi", "IV Ramakrishnan", "Vikas Ashok"], "title": "Finding the Signal in the Noise: An Exploratory Study on Assessing the Effectiveness of AI and Accessibility Forums for Blind Users' Support Needs", "comment": "20 pages incl. references, 5 figures. Full paper submission to CHI 2026. IRB-approved semi-structured interview study with 14 blind participants", "summary": "Accessibility forums and, more recently, generative AI tools have become vital resources for blind users seeking solutions to computer-interaction issues and learning about new assistive technologies, screen reader features, tutorials, and software updates. Understanding user experiences with these resources is essential for identifying and addressing persistent support gaps. Towards this, we interviewed 14 blind users who regularly engage with forums and GenAI tools. Findings revealed that forums often overwhelm users with multiple overlapping topics, redundant or irrelevant content, and fragmented responses that must be mentally pieced together, increasing cognitive load. GenAI tools, while offering more direct assistance, introduce new barriers by producing unreliable answers, including overly verbose or fragmented guidance, fabricated information, and contradictory suggestions that fail to follow prompts, thereby heightening verification demands. Based on these insights, we outlined design opportunities to improve the reliability of assistive resources, aiming to provide blind users with more trustworthy and cognitively-manageable support."}
{"id": "2602.18630", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18630", "abs": "https://arxiv.org/abs/2602.18630", "authors": ["Monalika Padma Reddy", "Aruna Balasubramanian", "Jiawei Zhou", "Xiaojun Bi", "IV Ramakrishnan", "Vikas Ashok"], "title": "Lost in Instructions: Study of Blind Users' Experiences with DIY Manuals and AI-Rewritten Instructions for Assembly, Operation, and Troubleshooting of Tangible Products", "comment": "28 pages incl. references, 7 figures. Full paper submission to CHI 2026. IRB-approved semi-structured interview and usability study with 15 blind participants", "summary": "AI tools like ChatGPT and Be-My-AI are increasingly being used by blind individuals. Although prior work has explored their use in some Do-It-Yourself (DIY) tasks by blind individuals, little is known about how they use these tools and the available product-manual resources to assemble, operate, and troubleshoot physical or tangible products - tasks requiring spatial reasoning, structural understanding, and precise execution. We address this knowledge gap via an interview study and a usability study with blind participants, investigating how they leverage AI tools and product manuals for DIY tasks with physical products. Findings show that manuals are essential resources, but product-manual instructions are often inadequate for blind users. AI tools presently do not adequately address this insufficiency; in fact, we observed that they often exacerbate this issue with incomplete, incoherent, or misleading guidance. Lastly, we suggest improvements to AI tools for generating tailored instructions for blind users' DIY tasks involving tangible products."}
{"id": "2602.18759", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18759", "abs": "https://arxiv.org/abs/2602.18759", "authors": ["Chen Chen", "Haobo Lin", "Yuanbo Xu"], "title": "Towards Reliable Negative Sampling for Recommendation with Implicit Feedback via In-Community Popularity", "comment": "12 pages, 9 figures", "summary": "Learning from implicit feedback is a fundamental problem in modern recommender systems, where only positive interactions are observed and explicit negative signals are unavailable. In such settings, negative sampling plays a critical role in model training by constructing negative items that enable effective preference learning and ranking optimization. However, designing reliable negative sampling strategies remains challenging, as they must simultaneously ensure realness, hardness, and interpretability. To this end, we propose \\textbf{ICPNS (In-Community Popularity Negative Sampling)}, a novel framework that leverages user community structure to identify reliable and informative negative samples. Our approach is grounded in the insight that item exposure is driven by latent user communities. By identifying these communities and utilizing in-community popularity, ICPNS effectively approximates the probability of item exposure. Consequently, items that are popular within a user's community but remain unclicked are identified as more reliable true negatives. Extensive experiments on four benchmark datasets demonstrate that ICPNS yields consistent improvements on graph-based recommenders and competitive performance on MF-based models, outperforming representative negative sampling strategies under a unified evaluation protocol."}
{"id": "2602.18807", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18807", "abs": "https://arxiv.org/abs/2602.18807", "authors": ["Eason Chen", "Sophia Judicke", "Kayla Beigh", "Xinyi Tang", "Isabel Wang", "Nina Yuan", "Zimo Xiao", "Chuangji Li", "Shizhuo Li", "Reed Luttmer", "Shreya Singh", "Maria Yampolsky", "Naman Parikh", "Yvonne Zhao", "Meiyi Chen", "Scarlett Huang", "Anishka Mohanty", "Gregory Johnson", "John Mackey", "Jionghao Lin", "Ken Koedinger"], "title": "Chat-Based Support Alone May Not Be Enough: Comparing Conversational and Embedded LLM Feedback for Mathematical Proof Learning", "comment": "15 pages, 4 figures, accepted at AIED 2025", "summary": "We evaluate GPTutor, an LLM-powered tutoring system for an undergraduate discrete mathematics course. It integrates two LLM-supported tools: a structured proof-review tool that provides embedded feedback on students' written proof attempts, and a chatbot for math questions. In a staggered-access study with 148 students, earlier access was associated with higher homework performance during the interval when only the experimental group could use the system, while we did not observe this performance increase transfer to exam scores. Usage logs show that students with lower self-efficacy and prior exam performance used both components more frequently. Session-level behavioral labels, produced by human coding and scaled using an automated classifier, characterize how students engaged with the chatbot (e.g., answer-seeking or help-seeking). In models controlling for prior performance and self-efficacy, higher chatbot usage and answer-seeking behavior were negatively associated with subsequent midterm performance, whereas proof-review usage showed no detectable independent association. Together, the findings suggest that chatbot-based support alone may not reliably support transfer to independent assessment of math proof-learning outcomes, whereas work-anchored, structured feedback appears less associated with reduced learning."}
{"id": "2602.18824", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18824", "abs": "https://arxiv.org/abs/2602.18824", "authors": ["Pedram Riyazimehr", "Seyyed Ehsan Mahmoudi"], "title": "UniRank: A Multi-Agent Calibration Pipeline for Estimating University Rankings from Anonymized Bibliometric Signals", "comment": null, "summary": "We present UniRank, a multi-agent LLM pipeline that estimates university positions across global ranking systems using only publicly available bibliometric data from OpenAlex and Semantic Scholar. The system employs a three-stage architecture: (a) zero-shot estimation from anonymized institutional metrics, (b) per-system tool-augmented calibration against real ranked universities, and (c) final synthesis. Critically, institutions are anonymized -- names, countries, DOIs, paper titles, and collaboration countries are all redacted -- and their actual ranks are hidden from the calibration tools during evaluation, preventing LLM memorization from confounding results. On the Times Higher Education (THE) World University Rankings ($n=352$), the system achieves MAE = 251.5 rank positions, Median AE = 131.5, PNMAE = 12.03%, Spearman $ρ= 0.769$, Kendall $τ= 0.591$, hit rate @50 = 20.7%, hit rate @100 = 39.8%, and a Memorization Index of exactly zero (no exact-match zero-width predictions among all 352 universities). The systematic positive-signed error (+190.1 positions, indicating the system consistently predicts worse ranks than actual) and monotonic performance degradation from elite tier (MAE = 60.5, hit@100 = 90.5%) to tail tier (MAE = 328.2, hit@100 = 20.8%) provide strong evidence that the pipeline performs genuine analytical reasoning rather than recalling memorized rankings. A live demo is available at https://unirank.scinito.ai ."}
{"id": "2602.18832", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.18832", "abs": "https://arxiv.org/abs/2602.18832", "authors": ["Eason Chen", "Ce Guan", "Ahmed Elshafiey", "Zhonghao Zhao", "Joshua Zekeri", "Afeez Edeifo Shaibu", "Emmanuel Osadebe Prince", "Cyuan Jhen Wu"], "title": "OpenClaw AI Agents as Informal Learners at Moltbook: Characterizing an Emergent Learning Community at Scale", "comment": "10 Pages", "summary": "Informal learning communities have been called the \"other Massive Open Online C\" in Learning@Scale research, yet remain understudied compared to MOOCs. We present the first empirical study of a large-scale informal learning community composed entirely of AI agents. Moltbook, a social network exclusively for AI agents powered by autonomous agent frameworks such as OpenClaw, grew to over 2.8 million registered agents in three weeks. Analyzing 231,080 non-spam posts across three phases of community evolution, we find three key patterns. First, participation inequality is extreme from the start (comment Gini = 0.889), exceeding human community benchmarks. Second, AI agents exhibit a \"broadcasting inversion\": statement-to-question ratios of 8.9:1 to 9.7:1 contrast sharply with the question-driven dynamics of human learning communities, and comment-level analysis of 1.55 million comments reveals a \"parallel monologue\" pattern where 93% of comments are independent responses rather than threaded dialogue. Third, we document a characteristic engagement lifecycle: explosive initial growth (184K posts from 32K authors in 11 days), a spam crisis (57,093 posts deleted by the platform), and engagement decline (mean comments: 31.7 -> 8.3 -> 1.7) that had not reversed by the end of our observation window despite effective spam removal. Sentiment analysis reveals a selection effect: comment tone becomes more positive as engagement declines, suggesting that casual participants disengage first while committed contributors remain. These findings have direct implications for hybrid human-AI learning platforms."}
{"id": "2602.18929", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18929", "abs": "https://arxiv.org/abs/2602.18929", "authors": ["Fuyuan Lyu", "Chenglin Luo", "Qiyuan Zhang", "Yupeng Hou", "Haolun Wu", "Xing Tang", "Xue Liu", "Jin L. C. Guo", "Xiuqiang He"], "title": "Give Users the Wheel: Towards Promptable Recommendation Paradigm", "comment": null, "summary": "Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios."}
{"id": "2602.18962", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.18962", "abs": "https://arxiv.org/abs/2602.18962", "authors": ["Albert Tang", "Yifan Mo", "Jie Li", "Yue Su", "Mengyuan Zhang", "Sander L. Koole", "Koen Hindriks", "Jiahuan Pei"], "title": "NeuroWise: A Multi-Agent LLM \"Glass-Box\" System for Practicing Double-Empathy Communication with Autistic Partners", "comment": "Accepted to ACM CHI 2026", "summary": "The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic \"deficits\" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual."}
{"id": "2602.19040", "categories": ["cs.IR", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.19040", "abs": "https://arxiv.org/abs/2602.19040", "authors": ["Jiaxin Wu", "Xiao-Yong Wei", "Qing Li"], "title": "Adaptive Multi-Agent Reasoning for Text-to-Video Retrieval", "comment": null, "summary": "The rise of short-form video platforms and the emergence of multimodal large language models (MLLMs) have amplified the need for scalable, effective, zero-shot text-to-video retrieval systems. While recent advances in large-scale pretraining have improved zero-shot cross-modal alignment, existing methods still struggle with query-dependent temporal reasoning, limiting their effectiveness on complex queries involving temporal, logical, or causal relationships. To address these limitations, we propose an adaptive multi-agent retrieval framework that dynamically orchestrates specialized agents over multiple reasoning iterations based on the demands of each query. The framework includes: (1) a retrieval agent for scalable retrieval over large video corpora, (2) a reasoning agent for zero-shot contextual temporal reasoning, and (3) a query reformulation agent for refining ambiguous queries and recovering performance for those that degrade over iterations. These agents are dynamically coordinated by an orchestration agent, which leverages intermediate feedback and reasoning outcomes to guide execution. We also introduce a novel communication mechanism that incorporates retrieval-performance memory and historical reasoning traces to improve coordination and decision-making. Experiments on three TRECVid benchmarks spanning eight years show that our framework achieves a twofold improvement over CLIP4Clip and significantly outperforms state-of-the-art methods by a large margin."}
{"id": "2602.19463", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.19463", "abs": "https://arxiv.org/abs/2602.19463", "authors": ["Emma Jiren Wang", "Siying Hu", "Zhicong Lu"], "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives", "comment": "19 pages, 8 figures; Accepted by ACM CHI 2026. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI'24)", "summary": "As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories."}
{"id": "2602.19629", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19629", "abs": "https://arxiv.org/abs/2602.19629", "authors": ["Tatia Codreanu"], "title": "Cooperation After the Algorithm: Designing Human-AI Coexistence Beyond the Illusion of Collaboration", "comment": "11 pages, 2 tables", "summary": "Generative artificial intelligence systems increasingly participate in research, law, education, media, and governance. Their fluent and adaptive outputs create an experience of collaboration. However, these systems do not bear responsibility, incur liability, or share stakes in downstream consequences. This structural asymmetry has already produced sanctions, professional errors, and governance failures in high-stakes contexts We argue that stable human-AI coexistence is an institutional achievement that depends on governance infrastructure capable of distributing residual risk. Drawing on institutional analysis and evolutionary cooperation theory, we introduce a formal inequality that specifies when reliance on AI yields positive expected cooperative value. The model makes explicit how governance conditions, system policy, and accountability regimes jointly determine whether cooperation is rational or structurally defective. From this formalization we derive a cooperation ecology framework with six design principles: reciprocity contracts, visible trust infrastructure, conditional cooperation modes, defection-mitigation mechanisms, narrative literacy against authority theatre, and an Earth-first sustainability constraint. We operationalize the framework through three policy artefacts: a Human-AI Cooperation Charter, a Defection Risk Register, and a Cooperation Readiness Audit. Together, these elements shift the unit of analysis from the user-AI dyad to the institutional environment that shapes incentives, signals, accountability, and repair. The paper provides a theoretical foundation and practical toolkit for designing human-AI systems that can sustain accountable, trustworthy cooperation over time."}
{"id": "2602.19702", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.19702", "abs": "https://arxiv.org/abs/2602.19702", "authors": ["Adamya Shyam", "Venkateswara Rao Kagita", "Bharti Rana", "Vikas Kumar"], "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework", "comment": null, "summary": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets."}
